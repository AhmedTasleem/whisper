{
    "summary": "The code handles speech transcription with Whisper model libraries, providing customizable parameters and managing segments, silence, and seek values. It transcribes audio files with timestamps, punctuation, and function options while checking input language and detecting hallucination using silence thresholds.",
    "details": [
        {
            "comment": "This code is importing necessary libraries and modules for the Whisper model to transcribe audio. It defines a function `transcribe` that takes in an audio file, optional arguments for verbose, temperature, compression_ratio_threshold, logprob_threshold, and no_speech_threshold. The function uses the Whisper model to transcribe the given audio.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":0-45",
            "content": "import argparse\nimport os\nimport traceback\nimport warnings\nfrom typing import TYPE_CHECKING, List, Optional, Tuple, Union\nimport numpy as np\nimport torch\nimport tqdm\nfrom .audio import (\n    FRAMES_PER_SECOND,\n    HOP_LENGTH,\n    N_FRAMES,\n    N_SAMPLES,\n    SAMPLE_RATE,\n    log_mel_spectrogram,\n    pad_or_trim,\n)\nfrom .decoding import DecodingOptions, DecodingResult\nfrom .timing import add_word_timestamps\nfrom .tokenizer import LANGUAGES, TO_LANGUAGE_CODE, get_tokenizer\nfrom .utils import (\n    exact_div,\n    format_timestamp,\n    get_end,\n    get_writer,\n    make_safe,\n    optional_float,\n    optional_int,\n    str2bool,\n)\nif TYPE_CHECKING:\n    from .model import Whisper\ndef transcribe(\n    model: \"Whisper\",\n    audio: Union[str, np.ndarray, torch.Tensor],\n    *,\n    verbose: Optional[bool] = None,\n    temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    compression_ratio_threshold: Optional[float] = 2.4,\n    logprob_threshold: Optional[float] = -1.0,\n    no_speech_threshold: Optional[float] = 0.6,"
        },
        {
            "comment": "This function transcribes an audio file using Whisper. It takes parameters like the model instance, audio file path or waveform, verbosity level for displaying text being decoded, and temperatures for sampling during the transcription process.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":46-72",
            "content": "    condition_on_previous_text: bool = True,\n    initial_prompt: Optional[str] = None,\n    word_timestamps: bool = False,\n    prepend_punctuations: str = \"\\\"'\u201c\u00bf([{-\",\n    append_punctuations: str = \"\\\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\",\n    clip_timestamps: Union[str, List[float]] = \"0\",\n    hallucination_silence_threshold: Optional[float] = None,\n    **decode_options,\n):\n    \"\"\"\n    Transcribe an audio file using Whisper\n    Parameters\n    ----------\n    model: Whisper\n        The Whisper model instance\n    audio: Union[str, np.ndarray, torch.Tensor]\n        The path to the audio file to open, or the audio waveform\n    verbose: bool\n        Whether to display the text being decoded to the console. If True, displays all the details,\n        If False, displays minimal details. If None, does not display anything\n    temperature: Union[float, Tuple[float, ...]]\n        Temperature for sampling. It can be a tuple of temperatures, which will be successively used\n        upon failures according to either `compression_ratio_threshold` or `logprob_threshold`."
        },
        {
            "comment": "These variables define the thresholds and conditions for determining if a segment is successful or not, whether to condition on previous text, extract word-level timestamps, and how to handle punctuation symbols in the output.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":74-94",
            "content": "    compression_ratio_threshold: float\n        If the gzip compression ratio is above this value, treat as failed\n    logprob_threshold: float\n        If the average log probability over sampled tokens is below this value, treat as failed\n    no_speech_threshold: float\n        If the no_speech probability is higher than this value AND the average log probability\n        over sampled tokens is below `logprob_threshold`, consider the segment as silent\n    condition_on_previous_text: bool\n        if True, the previous output of the model is provided as a prompt for the next window;\n        disabling may make the text inconsistent across windows, but the model becomes less prone to\n        getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n    word_timestamps: bool\n        Extract word-level timestamps using the cross-attention pattern and dynamic time warping,\n        and include the timestamps for each word in each segment.\n    prepend_punctuations: str\n        If word_timestamps is True, merge these punctuation symbols with the next word"
        },
        {
            "comment": "This function takes optional parameters for prompt, decoding options, clip timestamps, and hallucination silence threshold to transcribe speech into text. It returns a dictionary containing the resulting text and segment-level details.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":96-117",
            "content": "    append_punctuations: str\n        If word_timestamps is True, merge these punctuation symbols with the previous word\n    initial_prompt: Optional[str]\n        Optional text to provide as a prompt for the first window. This can be used to provide, or\n        \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n        to make it more likely to predict those word correctly.\n    decode_options: dict\n        Keyword arguments to construct `DecodingOptions` instances\n    clip_timestamps: Union[str, List[float]]\n        Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process.\n        The last end timestamp defaults to the end of the file.\n    hallucination_silence_threshold: Optional[float]\n        When word_timestamps is True, skip silent periods longer than this threshold (in seconds)\n        when a possible hallucination is detected\n    Returns\n    -------\n    A dictionary containing the resulting text (\"text\") and segment-level details (\"segments\"), and"
        },
        {
            "comment": "This code snippet is preparing the audio input for inference by handling the data type, padding silence to the input audio, and setting the language for decoding.\nThe code checks if the device is CPU or GPU and adjusts the data type accordingly. It also adds 30 seconds of silence to the input audio for slicing and calculates the content duration based on the frame size, hop length, and sample rate. If no language is specified in the decode options, it sets a default language if the model is not multilingual or displays a message if the model is multilingual.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":118-141",
            "content": "    the spoken language (\"language\"), which is detected when `decode_options[\"language\"]` is None.\n    \"\"\"\n    dtype = torch.float16 if decode_options.get(\"fp16\", True) else torch.float32\n    if model.device == torch.device(\"cpu\"):\n        if torch.cuda.is_available():\n            warnings.warn(\"Performing inference on CPU when CUDA is available\")\n        if dtype == torch.float16:\n            warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n            dtype = torch.float32\n    if dtype == torch.float32:\n        decode_options[\"fp16\"] = False\n    # Pad 30-seconds of silence to the input audio, for slicing\n    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n    content_frames = mel.shape[-1] - N_FRAMES\n    content_duration = float(content_frames * HOP_LENGTH / SAMPLE_RATE)\n    if decode_options.get(\"language\", None) is None:\n        if not model.is_multilingual:\n            decode_options[\"language\"] = \"en\"\n        else:\n            if verbose:\n                print("
        },
        {
            "comment": "The code is detecting the language of an audio clip using the first 30 seconds and then using that detected language to decide which tokenizer to use for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":142-166",
            "content": "                    \"Detecting language using up to the first 30 seconds. Use `--language` to specify the language\"\n                )\n            mel_segment = pad_or_trim(mel, N_FRAMES).to(model.device).to(dtype)\n            _, probs = model.detect_language(mel_segment)\n            decode_options[\"language\"] = max(probs, key=probs.get)\n            if verbose is not None:\n                print(\n                    f\"Detected language: {LANGUAGES[decode_options['language']].title()}\"\n                )\n    language: str = decode_options[\"language\"]\n    task: str = decode_options.get(\"task\", \"transcribe\")\n    tokenizer = get_tokenizer(\n        model.is_multilingual,\n        num_languages=model.num_languages,\n        language=language,\n        task=task,\n    )\n    if isinstance(clip_timestamps, str):\n        clip_timestamps = [\n            float(ts) for ts in (clip_timestamps.split(\",\") if clip_timestamps else [])\n        ]\n    seek_points: List[int] = [round(ts * FRAMES_PER_SECOND) for ts in clip_timestamps]\n    if len(seek_points) == 0:"
        },
        {
            "comment": "This code segment is preparing data for decoding a model. It creates seek points, a list of punctuation, and sets up temperature values for the decoder. If word-level timestamps are present and the task is translation, it warns about potential unreliability. The function `decode_with_fallback` is defined to decode the segment with different temperatures using the specified decoding options and model.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":167-194",
            "content": "        seek_points.append(0)\n    if len(seek_points) % 2 == 1:\n        seek_points.append(content_frames)\n    seek_clips: List[Tuple[int, int]] = list(zip(seek_points[::2], seek_points[1::2]))\n    punctuation = \"\\\"'\u201c\u00bf([{-\\\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\"\n    if word_timestamps and task == \"translate\":\n        warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n    def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n        temperatures = (\n            [temperature] if isinstance(temperature, (int, float)) else temperature\n        )\n        decode_result = None\n        for t in temperatures:\n            kwargs = {**decode_options}\n            if t > 0:\n                # disable beam_size and patience when t > 0\n                kwargs.pop(\"beam_size\", None)\n                kwargs.pop(\"patience\", None)\n            else:\n                # disable best_of when t == 0\n                kwargs.pop(\"best_of\", None)\n            options = DecodingOptions(**kwargs, temperature=t)\n            decode_result = model.decode(segment, options)"
        },
        {
            "comment": "This code checks if the decoding result requires a fallback, based on three conditions: compression ratio, average log probability, and silence detection. If no fallback is needed, it breaks out of the loop. The code then calculates the number of mel frames per output token and time precision for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":196-224",
            "content": "            needs_fallback = False\n            if (\n                compression_ratio_threshold is not None\n                and decode_result.compression_ratio > compression_ratio_threshold\n            ):\n                needs_fallback = True  # too repetitive\n            if (\n                logprob_threshold is not None\n                and decode_result.avg_logprob < logprob_threshold\n            ):\n                needs_fallback = True  # average log probability is too low\n            if (\n                no_speech_threshold is not None\n                and decode_result.no_speech_prob > no_speech_threshold\n            ):\n                needs_fallback = False  # silence\n            if not needs_fallback:\n                break\n        return decode_result\n    clip_idx = 0\n    seek = seek_clips[clip_idx][0]\n    input_stride = exact_div(\n        N_FRAMES, model.dims.n_audio_ctx\n    )  # mel frames per output token: 2\n    time_precision = (\n        input_stride * HOP_LENGTH / SAMPLE_RATE\n    )  # time per output token: 0.02 (seconds)"
        },
        {
            "comment": "This code segment initializes variables for storing tokens and segments, handles an optional initial prompt, defines a function for creating new speech segments, and sets up a progress bar.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":225-254",
            "content": "    all_tokens = []\n    all_segments = []\n    prompt_reset_since = 0\n    if initial_prompt is not None:\n        initial_prompt_tokens = tokenizer.encode(\" \" + initial_prompt.strip())\n        all_tokens.extend(initial_prompt_tokens)\n    else:\n        initial_prompt_tokens = []\n    def new_segment(\n        *, start: float, end: float, tokens: torch.Tensor, result: DecodingResult\n    ):\n        tokens = tokens.tolist()\n        text_tokens = [token for token in tokens if token < tokenizer.eot]\n        return {\n            \"seek\": seek,\n            \"start\": start,\n            \"end\": end,\n            \"text\": tokenizer.decode(text_tokens),\n            \"tokens\": tokens,\n            \"temperature\": result.temperature,\n            \"avg_logprob\": result.avg_logprob,\n            \"compression_ratio\": result.compression_ratio,\n            \"no_speech_prob\": result.no_speech_prob,\n        }\n    # show the progress bar when verbose is False (if True, transcribed text will be printed)\n    with tqdm.tqdm(\n        total=content_frames, unit=\"frames\", disable=verbose is not False"
        },
        {
            "comment": "Iterates over seek clips, finding the next segment to transcribe based on seek time and clip boundaries.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":255-274",
            "content": "    ) as pbar:\n        last_speech_timestamp = 0.0\n        # NOTE: This loop is obscurely flattened to make the diff readable.\n        # A later commit should turn this into a simpler nested loop.\n        # for seek_clip_start, seek_clip_end in seek_clips:\n        #     while seek < seek_clip_end\n        while clip_idx < len(seek_clips):\n            seek_clip_start, seek_clip_end = seek_clips[clip_idx]\n            if seek < seek_clip_start:\n                seek = seek_clip_start\n            if seek >= seek_clip_end:\n                clip_idx += 1\n                if clip_idx < len(seek_clips):\n                    seek = seek_clips[clip_idx][0]\n                continue\n            time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n            window_end_time = float((seek + N_FRAMES) * HOP_LENGTH / SAMPLE_RATE)\n            segment_size = min(N_FRAMES, content_frames - seek, seek_clip_end - seek)\n            mel_segment = mel[:, seek : seek + segment_size]\n            segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE"
        },
        {
            "comment": "This code segment is responsible for speech recognition and processing in the whisper library. It takes an input audio segment, performs speech recognition using a model, and checks if there is any no-speech activity or anomalous words. If any of these conditions are met, it skips the current segment and moves to the next one. The code also keeps track of the seek position (seek) for fast forwarding to the next segment boundary.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":275-299",
            "content": "            mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n            decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n            result: DecodingResult = decode_with_fallback(mel_segment)\n            tokens = torch.tensor(result.tokens)\n            if no_speech_threshold is not None:\n                # no voice activity check\n                should_skip = result.no_speech_prob > no_speech_threshold\n                if (\n                    logprob_threshold is not None\n                    and result.avg_logprob > logprob_threshold\n                ):\n                    # don't skip if the logprob is high enough, despite the no_speech_prob\n                    should_skip = False\n                if should_skip:\n                    seek += segment_size  # fast-forward to the next segment boundary\n                    continue\n            previous_seek = seek\n            current_segments = []\n            # anomalous words are very long/short/improbable\n            def word_anomaly_score(word: dict) -> float:"
        },
        {
            "comment": "This code calculates a score for each segment based on word probability and duration, checks if a segment is anomalous by considering its words and returns the next non-empty segment from a list. It also handles timestamp tokens.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":300-322",
            "content": "                probability = word.get(\"probability\", 0.0)\n                duration = word[\"end\"] - word[\"start\"]\n                score = 0.0\n                if probability < 0.15:\n                    score += 1.0\n                if duration < 0.133:\n                    score += (0.133 - duration) * 15\n                if duration > 2.0:\n                    score += duration - 2.0\n                return score\n            def is_segment_anomaly(segment: Optional[dict]) -> bool:\n                if segment is None or not segment[\"words\"]:\n                    return False\n                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\n                words = words[:8]\n                score = sum(word_anomaly_score(w) for w in words)\n                return score >= 3 or score + 0.01 >= len(words)\n            def next_words_segment(segments: List[dict]) -> Optional[dict]:\n                return next((s for s in segments if s[\"words\"]), None)\n            timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)"
        },
        {
            "comment": "This code checks for consecutive timestamp tokens and extracts corresponding segments from the input sequence of tokens. If a segment is found, it calculates the start and end timestamp positions and creates a new segment with adjusted time values.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":323-344",
            "content": "            single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n            consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n            consecutive.add_(1)\n            if len(consecutive) > 0:\n                # if the output contains two consecutive timestamp tokens\n                slices = consecutive.tolist()\n                if single_timestamp_ending:\n                    slices.append(len(tokens))\n                last_slice = 0\n                for current_slice in slices:\n                    sliced_tokens = tokens[last_slice:current_slice]\n                    start_timestamp_pos = (\n                        sliced_tokens[0].item() - tokenizer.timestamp_begin\n                    )\n                    end_timestamp_pos = (\n                        sliced_tokens[-1].item() - tokenizer.timestamp_begin\n                    )\n                    current_segments.append(\n                        new_segment(\n                            start=time_offset + start_timestamp_pos * time_precision,"
        },
        {
            "comment": "Code is parsing audio segments and extracting relevant data for further processing. It seeks to the next timestamp or ignores unfinished segments if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":345-366",
            "content": "                            end=time_offset + end_timestamp_pos * time_precision,\n                            tokens=sliced_tokens,\n                            result=result,\n                        )\n                    )\n                    last_slice = current_slice\n                if single_timestamp_ending:\n                    # single timestamp at the end means no speech after the last timestamp.\n                    seek += segment_size\n                else:\n                    # otherwise, ignore the unfinished segment and seek to the last timestamp\n                    last_timestamp_pos = (\n                        tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n                    )\n                    seek += last_timestamp_pos * input_stride\n            else:\n                duration = segment_duration\n                timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n                if (\n                    len(timestamps) > 0\n                    and timestamps[-1].item() != tokenizer.timestamp_begin"
        },
        {
            "comment": "This code segment is responsible for creating speech segments based on the given timestamps and tokens, and then adding word timestamps if necessary. If consecutive timestamps are not found but a timestamp exists, it uses the last one to determine the duration of the current segment. It appends the created segment to the current_segments list, and increments the seek value for the next iteration.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":367-392",
            "content": "                ):\n                    # no consecutive timestamps but it has a timestamp; use the last one.\n                    last_timestamp_pos = (\n                        timestamps[-1].item() - tokenizer.timestamp_begin\n                    )\n                    duration = last_timestamp_pos * time_precision\n                current_segments.append(\n                    new_segment(\n                        start=time_offset,\n                        end=time_offset + duration,\n                        tokens=tokens,\n                        result=result,\n                    )\n                )\n                seek += segment_size\n            if word_timestamps:\n                add_word_timestamps(\n                    segments=current_segments,\n                    model=model,\n                    tokenizer=tokenizer,\n                    mel=mel_segment,\n                    num_frames=segment_size,\n                    prepend_punctuations=prepend_punctuations,\n                    append_punctuations=append_punctuations,"
        },
        {
            "comment": "This code skips silence before possible hallucinations and sets seek position accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":393-411",
            "content": "                    last_speech_timestamp=last_speech_timestamp,\n                )\n                if not single_timestamp_ending:\n                    last_word_end = get_end(current_segments)\n                    if last_word_end is not None and last_word_end > time_offset:\n                        seek = round(last_word_end * FRAMES_PER_SECOND)\n                # skip silence before possible hallucinations\n                if hallucination_silence_threshold is not None:\n                    threshold = hallucination_silence_threshold\n                    if not single_timestamp_ending:\n                        last_word_end = get_end(current_segments)\n                        if last_word_end is not None and last_word_end > time_offset:\n                            remaining_duration = window_end_time - last_word_end\n                            if remaining_duration > threshold:\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\n                            else:\n                                seek = previous_seek + segment_size"
        },
        {
            "comment": "This code skips silence before possible hallucinations that are surrounded by silence or more hallucinations.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":413-430",
            "content": "                    # if first segment might be a hallucination, skip leading silence\n                    first_segment = next_words_segment(current_segments)\n                    if first_segment is not None and is_segment_anomaly(first_segment):\n                        gap = first_segment[\"start\"] - time_offset\n                        if gap > threshold:\n                            seek = previous_seek + round(gap * FRAMES_PER_SECOND)\n                            continue\n                    # skip silence before any possible hallucination that is surrounded\n                    # by silence or more hallucinations\n                    hal_last_end = last_speech_timestamp\n                    for si in range(len(current_segments)):\n                        segment = current_segments[si]\n                        if not segment[\"words\"]:\n                            continue\n                        if is_segment_anomaly(segment):\n                            next_segment = next_words_segment(\n                                current_segments[si + 1 :]"
        },
        {
            "comment": "If the next segment exists, set hal_next_start to the start time of the first word in that segment. Otherwise, set it to time_offset plus segment_duration. Determine if there is silence before and after this segment based on certain conditions, such as segment start times or anomalies. If both silences are present, round the seek value to be the maximum between time_offset + 1 and the segment's start time.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":431-448",
            "content": "                            )\n                            if next_segment is not None:\n                                hal_next_start = next_segment[\"words\"][0][\"start\"]\n                            else:\n                                hal_next_start = time_offset + segment_duration\n                            silence_before = (\n                                segment[\"start\"] - hal_last_end > threshold\n                                or segment[\"start\"] < threshold\n                                or segment[\"start\"] - time_offset < 2.0\n                            )\n                            silence_after = (\n                                hal_next_start - segment[\"end\"] > threshold\n                                or is_segment_anomaly(next_segment)\n                                or window_end_time - segment[\"end\"] < 2.0\n                            )\n                            if silence_before and silence_after:\n                                seek = round(\n                                    max(time_offset + 1, segment[\"start\"])"
        },
        {
            "comment": "Removes small segments and instantaneous segments without text from the current_segments list.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":449-469",
            "content": "                                    * FRAMES_PER_SECOND\n                                )\n                                if content_duration - segment[\"end\"] < threshold:\n                                    seek = content_frames\n                                current_segments[si:] = []\n                                break\n                        hal_last_end = segment[\"end\"]\n                last_word_end = get_end(current_segments)\n                if last_word_end is not None:\n                    last_speech_timestamp = last_word_end\n            if verbose:\n                for segment in current_segments:\n                    start, end, text = segment[\"start\"], segment[\"end\"], segment[\"text\"]\n                    line = f\"[{format_timestamp(start)} --> {format_timestamp(end)}] {text}\"\n                    print(make_safe(line))\n            # if a segment is instantaneous or does not contain text, clear it\n            for i, segment in enumerate(current_segments):\n                if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":"
        },
        {
            "comment": "1. Initializes empty text, tokens, and words for segments.\n2. Appends segments to all_segments list with unique IDs.\n3. Extends all_tokens list by combining segment's tokens.\n4. Checks if condition on previous text or temperature is high; resets prompt_reset_since if true.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":470-501",
            "content": "                    segment[\"text\"] = \"\"\n                    segment[\"tokens\"] = []\n                    segment[\"words\"] = []\n            all_segments.extend(\n                [\n                    {\"id\": i, **segment}\n                    for i, segment in enumerate(\n                        current_segments, start=len(all_segments)\n                    )\n                ]\n            )\n            all_tokens.extend(\n                [token for segment in current_segments for token in segment[\"tokens\"]]\n            )\n            if not condition_on_previous_text or result.temperature > 0.5:\n                # do not feed the prompt tokens if a high temperature was used\n                prompt_reset_since = len(all_tokens)\n            # update progress bar\n            pbar.update(min(content_frames, seek) - previous_seek)\n    return dict(\n        text=tokenizer.decode(all_tokens[len(initial_prompt_tokens) :]),\n        segments=all_segments,\n        language=language,\n    )\ndef cli():\n    from . import available_models"
        },
        {
            "comment": "The function 'valid_model_name' checks if the input name exists in available models or is a valid path to a model checkpoint. If so, it returns the name. Else, it raises a ValueError with an error message.\n\nThe parser object is initialized for parsing command line arguments. It accepts audio file(s) to transcribe as input and allows specifying various options like model name, model directory, device for PyTorch inference, and output directory for saving the outputs. The default options are set based on system availability.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":503-517",
            "content": "    def valid_model_name(name):\n        if name in available_models() or os.path.exists(name):\n            return name\n        raise ValueError(\n            f\"model should be one of {available_models()} or path to a model checkpoint\"\n        )\n    # fmt: off\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"audio\", nargs=\"+\", type=str, help=\"audio file(s) to transcribe\")\n    parser.add_argument(\"--model\", default=\"small\", type=valid_model_name, help=\"name of the Whisper model to use\")\n    parser.add_argument(\"--model_dir\", type=str, default=None, help=\"the path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n    parser.add_argument(\"--output_format\", \"-f\", type=str, default=\"all\", choi"
        },
        {
            "comment": "This code is parsing command line arguments for a transcribe and translation task. It sets default values and provides options to choose the output file format, enable verbose output, perform speech recognition or translation, specify input language, set temperature for sampling, and set beam size for beam search.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":517-525",
            "content": "ces=[\"txt\", \"vtt\", \"srt\", \"tsv\", \"json\", \"all\"], help=\"format of the output file; if not specified, all available formats will be produced\")\n    parser.add_argument(\"--verbose\", type=str2bool, default=True, help=\"whether to print out the progress and debug messages\")\n    parser.add_argument(\"--task\", type=str, default=\"transcribe\", choices=[\"transcribe\", \"translate\"], help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=None, choices=sorted(LANGUAGES.keys()) + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]), help=\"language spoken in the audio, specify None to perform language detection\")\n    parser.add_argument(\"--temperature\", type=float, default=0, help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=5, help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=5, help=\"number of beams in beam search, only applicable when temperature is zero\")"
        },
        {
            "comment": "This code adds command line arguments to a parser, allowing the user to set optional values for patience, length penalty, tokens to suppress during sampling, initial prompt, and conditioning on previous text.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":526-531",
            "content": "    parser.add_argument(\"--patience\", type=float, default=None, help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=None, help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default\")\n    parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=True, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")"
        },
        {
            "comment": "This code snippet is adding command-line arguments for various parameters used in a speech transcription process. The parameters include whether to perform inference in half precision (fp16), temperature increment when falling back, compression ratio threshold, log probability threshold, no speech probability threshold, and whether to use word timestamps. These options allow the user to customize how the transcribe function operates.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":532-538",
            "content": "    parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n    parser.add_argument(\"--word_timestamps\", type=str2bool,"
        },
        {
            "comment": "This code is adding optional arguments to a parser for various features related to word-level timestamps and punctuation handling. It includes options for enabling word timestamps, specifying punctuations to merge with next/previous words, highlighting spoken words in output files, and setting maximum line width and count in segments (requires --word_timestamps True).",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":538-544",
            "content": " default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'\u201c\u00bf([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n    parser.add_argument(\"--highlight_words\", type=str2bool, default=False, help=\"(requires --word_timestamps True) underline each word as it is spoken in srt and vtt\")\n    parser.add_argument(\"--max_line_width\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of characters in a line before breaking the line\")\n    parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")\n    parser.add_argument(\"--max_word"
        },
        {
            "comment": "The code defines command-line arguments for a transcribe function. It allows setting the maximum number of words in a segment, specifying the number of threads to use for CPU inference, defining clips to process with timestamps, and setting a silence threshold for hallucination detection. The code then parses these arguments into a dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":544-554",
            "content": "s_per_line\", type=optional_int, default=None, help=\"(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment\")\n    parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n    parser.add_argument(\"--clip_timestamps\", type=str, default=\"0\", help=\"comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process, where the last end timestamp defaults to the end of the file\")\n    parser.add_argument(\"--hallucination_silence_threshold\", type=optional_float, help=\"(requires --word_timestamps True) skip silent periods longer than this threshold (in seconds) when a possible hallucination is detected\")\n    # fmt: on\n    args = parser.parse_args().__dict__\n    model_name: str = args.pop(\"model\")\n    model_dir: str = args.pop(\"model_dir\")\n    output_dir: str = args.pop(\"output_dir\")\n    output_format: str = args.pop(\"output_format\")"
        },
        {
            "comment": "The code is setting up the model and its configuration for transcribing audio files. It checks if the received language matches the model's language, warns and defaults to English if not. It also sets the temperature and number of threads for model inference, loads the model, and gets a writer object for output. Additionally, it handles word options such as highlight words, maximum line count, maximum line width, and maximum words per line.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":555-585",
            "content": "    device: str = args.pop(\"device\")\n    os.makedirs(output_dir, exist_ok=True)\n    if model_name.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n        if args[\"language\"] is not None:\n            warnings.warn(\n                f\"{model_name} is an English-only model but receipted '{args['language']}'; using English instead.\"\n            )\n        args[\"language\"] = \"en\"\n    temperature = args.pop(\"temperature\")\n    if (increment := args.pop(\"temperature_increment_on_fallback\")) is not None:\n        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, increment))\n    else:\n        temperature = [temperature]\n    if (threads := args.pop(\"threads\")) > 0:\n        torch.set_num_threads(threads)\n    from . import load_model\n    model = load_model(model_name, device=device, download_root=model_dir)\n    writer = get_writer(output_format, output_dir)\n    word_options = [\n        \"highlight_words\",\n        \"max_line_count\",\n        \"max_line_width\",\n        \"max_words_per_line\",\n    ]\n    if not args[\"word_timestamps\"]:"
        },
        {
            "comment": "This code block checks if certain command line arguments are used correctly and handles any cases where they're not. It then passes the required arguments to the transcribe function and writes the results using a specified writer function.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/transcribe.py\":586-604",
            "content": "        for option in word_options:\n            if args[option]:\n                parser.error(f\"--{option} requires --word_timestamps True\")\n    if args[\"max_line_count\"] and not args[\"max_line_width\"]:\n        warnings.warn(\"--max_line_count has no effect without --max_line_width\")\n    if args[\"max_words_per_line\"] and args[\"max_line_width\"]:\n        warnings.warn(\"--max_words_per_line has no effect with --max_line_width\")\n    writer_args = {arg: args.pop(arg) for arg in word_options}\n    for audio_path in args.pop(\"audio\"):\n        try:\n            result = transcribe(model, audio_path, temperature=temperature, **args)\n            writer(result, audio_path, **writer_args)\n        except Exception as e:\n            traceback.print_exc()\n            print(f\"Skipping {audio_path} due to {type(e).__name__}: {str(e)}\")\nif __name__ == \"__main__\":\n    cli()"
        }
    ]
}