{
    "summary": "The Tokenizer class in the code uses tiktoken library for efficient text tokenization, supporting various languages and special tokens for different tasks such as translation and transcribe, while also handling speaker tags, non-speech annotations, EOT, SOT, and noise suppression for improved performance.",
    "details": [
        {
            "comment": "This code is defining a dictionary of language codes and their corresponding language names.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":0-50",
            "content": "import base64\nimport os\nimport string\nfrom dataclasses import dataclass, field\nfrom functools import cached_property, lru_cache\nfrom typing import Dict, List, Optional, Tuple\nimport tiktoken\nLANGUAGES = {\n    \"en\": \"english\",\n    \"zh\": \"chinese\",\n    \"de\": \"german\",\n    \"es\": \"spanish\",\n    \"ru\": \"russian\",\n    \"ko\": \"korean\",\n    \"fr\": \"french\",\n    \"ja\": \"japanese\",\n    \"pt\": \"portuguese\",\n    \"tr\": \"turkish\",\n    \"pl\": \"polish\",\n    \"ca\": \"catalan\",\n    \"nl\": \"dutch\",\n    \"ar\": \"arabic\",\n    \"sv\": \"swedish\",\n    \"it\": \"italian\",\n    \"id\": \"indonesian\",\n    \"hi\": \"hindi\",\n    \"fi\": \"finnish\",\n    \"vi\": \"vietnamese\",\n    \"he\": \"hebrew\",\n    \"uk\": \"ukrainian\",\n    \"el\": \"greek\",\n    \"ms\": \"malay\",\n    \"cs\": \"czech\",\n    \"ro\": \"romanian\",\n    \"da\": \"danish\",\n    \"hu\": \"hungarian\",\n    \"ta\": \"tamil\",\n    \"no\": \"norwegian\",\n    \"th\": \"thai\",\n    \"ur\": \"urdu\",\n    \"hr\": \"croatian\",\n    \"bg\": \"bulgarian\",\n    \"lt\": \"lithuanian\",\n    \"la\": \"latin\",\n    \"mi\": \"maori\",\n    \"ml\": \"malayalam\",\n    \"cy\": \"welsh\",\n    \"sk\": \"slovak\",\n    \"te\": \"telugu\","
        },
        {
            "comment": "This code contains a dictionary where keys are language codes and values are the corresponding language names. The languages represented here are diverse, covering various regions and scripts.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":51-100",
            "content": "    \"fa\": \"persian\",\n    \"lv\": \"latvian\",\n    \"bn\": \"bengali\",\n    \"sr\": \"serbian\",\n    \"az\": \"azerbaijani\",\n    \"sl\": \"slovenian\",\n    \"kn\": \"kannada\",\n    \"et\": \"estonian\",\n    \"mk\": \"macedonian\",\n    \"br\": \"breton\",\n    \"eu\": \"basque\",\n    \"is\": \"icelandic\",\n    \"hy\": \"armenian\",\n    \"ne\": \"nepali\",\n    \"mn\": \"mongolian\",\n    \"bs\": \"bosnian\",\n    \"kk\": \"kazakh\",\n    \"sq\": \"albanian\",\n    \"sw\": \"swahili\",\n    \"gl\": \"galician\",\n    \"mr\": \"marathi\",\n    \"pa\": \"punjabi\",\n    \"si\": \"sinhala\",\n    \"km\": \"khmer\",\n    \"sn\": \"shona\",\n    \"yo\": \"yoruba\",\n    \"so\": \"somali\",\n    \"af\": \"afrikaans\",\n    \"oc\": \"occitan\",\n    \"ka\": \"georgian\",\n    \"be\": \"belarusian\",\n    \"tg\": \"tajik\",\n    \"sd\": \"sindhi\",\n    \"gu\": \"gujarati\",\n    \"am\": \"amharic\",\n    \"yi\": \"yiddish\",\n    \"lo\": \"lao\",\n    \"uz\": \"uzbek\",\n    \"fo\": \"faroese\",\n    \"ht\": \"haitian creole\",\n    \"ps\": \"pashto\",\n    \"tk\": \"turkmen\",\n    \"nn\": \"nynorsk\",\n    \"mt\": \"maltese\",\n    \"sa\": \"sanskrit\",\n    \"lb\": \"luxembourgish\",\n    \"my\": \"myanmar\",\n    \"bo\": \"tibetan\",\n    \"tl\": \"tagalog\",\n    \"mg\": \"malagasy\","
        },
        {
            "comment": "This code defines a class called Tokenizer that wraps around the `tiktoken` library. It provides quick access to special tokens and supports specifying a language, task, and start-of-text (SOT) sequence. The code also includes a dictionary mapping language aliases to their respective language codes for easier lookup.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":101-143",
            "content": "    \"as\": \"assamese\",\n    \"tt\": \"tatar\",\n    \"haw\": \"hawaiian\",\n    \"ln\": \"lingala\",\n    \"ha\": \"hausa\",\n    \"ba\": \"bashkir\",\n    \"jw\": \"javanese\",\n    \"su\": \"sundanese\",\n    \"yue\": \"cantonese\",\n}\n# language code lookup by name, with a few language aliases\nTO_LANGUAGE_CODE = {\n    **{language: code for code, language in LANGUAGES.items()},\n    \"burmese\": \"my\",\n    \"valencian\": \"ca\",\n    \"flemish\": \"nl\",\n    \"haitian\": \"ht\",\n    \"letzeburgesch\": \"lb\",\n    \"pushto\": \"ps\",\n    \"panjabi\": \"pa\",\n    \"moldavian\": \"ro\",\n    \"moldovan\": \"ro\",\n    \"sinhalese\": \"si\",\n    \"castilian\": \"es\",\n    \"mandarin\": \"zh\",\n}\n@dataclass\nclass Tokenizer:\n    \"\"\"A thin wrapper around `tiktoken` providing quick access to special tokens\"\"\"\n    encoding: tiktoken.Encoding\n    num_languages: int\n    language: Optional[str] = None\n    task: Optional[str] = None\n    sot_sequence: Tuple[int] = ()\n    special_tokens: Dict[str, int] = field(default_factory=dict)\n    def __post_init__(self):\n        for special in self.encoding.special_tokens_set:\n            special_token = self.encoding.encode_single_token(special)"
        },
        {
            "comment": "This code defines a class for tokenizing text with special tokens, language information, and task information. It also includes methods to encode and decode tokenized text.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":144-168",
            "content": "            self.special_tokens[special] = special_token\n        sot: int = self.special_tokens[\"<|startoftranscript|>\"]\n        translate: int = self.special_tokens[\"<|translate|>\"]\n        transcribe: int = self.special_tokens[\"<|transcribe|>\"]\n        langs = tuple(LANGUAGES.keys())[: self.num_languages]\n        sot_sequence = [sot]\n        if self.language is not None:\n            sot_sequence.append(sot + 1 + langs.index(self.language))\n        if self.task is not None:\n            task_token: int = transcribe if self.task == \"transcribe\" else translate\n            sot_sequence.append(task_token)\n        self.sot_sequence = tuple(sot_sequence)\n    def encode(self, text, **kwargs):\n        return self.encoding.encode(text, **kwargs)\n    def decode(self, token_ids: List[int], **kwargs) -> str:\n        token_ids = [t for t in token_ids if t < self.timestamp_begin]\n        return self.encoding.decode(token_ids, **kwargs)\n    def decode_with_timestamps(self, token_ids: List[int], **kwargs) -> str:\n        \"\"\""
        },
        {
            "comment": "The code defines a tokenizer class with properties for various special tokens like end-of-transcript (eot), transcribe, translate, start of transcript (sot), start of language model (sot_lm), start of previous transcript (sot_prev), no speech, and no timestamps. The decode method decodes given tokens with timestamps annotated using the encoding's decode function.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":169-203",
            "content": "        Timestamp tokens are above other special tokens' id range and are ignored by `decode()`.\n        This method decodes given tokens with timestamps tokens annotated, e.g. \"<|1.08|>\".\n        \"\"\"\n        return self.encoding.decode(token_ids, **kwargs)\n    @cached_property\n    def eot(self) -> int:\n        return self.encoding.eot_token\n    @cached_property\n    def transcribe(self) -> int:\n        return self.special_tokens[\"<|transcribe|>\"]\n    @cached_property\n    def translate(self) -> int:\n        return self.special_tokens[\"<|translate|>\"]\n    @cached_property\n    def sot(self) -> int:\n        return self.special_tokens[\"<|startoftranscript|>\"]\n    @cached_property\n    def sot_lm(self) -> int:\n        return self.special_tokens[\"<|startoflm|>\"]\n    @cached_property\n    def sot_prev(self) -> int:\n        return self.special_tokens[\"<|startofprev|>\"]\n    @cached_property\n    def no_speech(self) -> int:\n        return self.special_tokens[\"<|nospeech|>\"]\n    @cached_property\n    def no_timestamps(self) -> int:"
        },
        {
            "comment": "This code is a part of a tokenizer class for a language model. It includes properties and methods to handle special tokens such as \"<|notimestamps|>\", timestamp tokens like \"<|0.00|>\", and language-specific tokens. The `to_language_token` method returns the token id corresponding to the input language, while `all_language_tokens` and `all_language_codes` return tuples of all language tokens and codes respectively.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":204-233",
            "content": "        return self.special_tokens[\"<|notimestamps|>\"]\n    @cached_property\n    def timestamp_begin(self) -> int:\n        return self.special_tokens[\"<|0.00|>\"]\n    @cached_property\n    def language_token(self) -> int:\n        \"\"\"Returns the token id corresponding to the value of the `language` field\"\"\"\n        if self.language is None:\n            raise ValueError(\"This tokenizer does not have language token configured\")\n        return self.to_language_token(self.language)\n    def to_language_token(self, language):\n        if token := self.special_tokens.get(f\"<|{language}|>\", None):\n            return token\n        raise KeyError(f\"Language {language} not found in tokenizer.\")\n    @cached_property\n    def all_language_tokens(self) -> Tuple[int]:\n        result = []\n        for token, token_id in self.special_tokens.items():\n            if token.strip(\"<|>\") in LANGUAGES:\n                result.append(token_id)\n        return tuple(result)[: self.num_languages]\n    @cached_property\n    def all_language_codes(self) -> Tuple[str]:"
        },
        {
            "comment": "This code defines a tokenizer class with various properties and methods for handling language tokens, speaker tags, non-speech annotations, and punctuations. It includes functions to decode language tokens, create a sequence of start of text (SOT) including no time stamps, and a list of non-speech tokens to suppress for accurate sampling of spoken audio.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":234-258",
            "content": "        return tuple(self.decode([_l]).strip(\"<|>\") for _l in self.all_language_tokens)\n    @cached_property\n    def sot_sequence_including_notimestamps(self) -> Tuple[int]:\n        return tuple(list(self.sot_sequence) + [self.no_timestamps])\n    @cached_property\n    def non_speech_tokens(self) -> Tuple[int]:\n        \"\"\"\n        Returns the list of tokens to suppress in order to avoid any speaker tags or non-speech\n        annotations, to prevent sampling texts that are not actually spoken in the audio, e.g.\n        - \u266a\u266a\u266a\n        - ( SPEAKING FOREIGN LANGUAGE )\n        - [DAVID] Hey there,\n        keeping basic punctuations like commas, periods, question marks, exclamation points, etc.\n        \"\"\"\n        symbols = list('\"#()*+/:;<=>@[\\\\]^_`{|}~\u300c\u300d\u300e\u300f')\n        symbols += (\n            \"<< >> <<< >>> -- --- -( -[ (' (\\\" (( )) ((( ))) [[ ]] {{ }} \u266a\u266a \u266a\u266a\u266a\".split()\n        )\n        # symbols that may be a single token or multiple tokens depending on the tokenizer.\n        # In case they're multiple tokens, suppress the first token, which is safe because:"
        },
        {
            "comment": "This function creates a set of characters to suppress and allows hyphens and single quotes between words. It then encodes various symbols, checks if they are 1 character long or in the miscellaneous set, and adds them to the result set. Finally, it sorts and returns the result set.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":259-278",
            "content": "        # These are between U+2640 and U+267F miscellaneous symbols that are okay to suppress\n        # in generations, and in the 3-byte UTF-8 representation they share the first two bytes.\n        miscellaneous = set(\"\u2669\u266a\u266b\u266c\u266d\u266e\u266f\")\n        assert all(0x2640 <= ord(c) <= 0x267F for c in miscellaneous)\n        # allow hyphens \"-\" and single quotes \"'\" between words, but not at the beginning of a word\n        result = {self.encoding.encode(\" -\")[0], self.encoding.encode(\" '\")[0]}\n        for symbol in symbols + list(miscellaneous):\n            for tokens in [\n                self.encoding.encode(symbol),\n                self.encoding.encode(\" \" + symbol),\n            ]:\n                if len(tokens) == 1 or symbol in miscellaneous:\n                    result.add(tokens[0])\n        return tuple(sorted(result))\n    def split_to_word_tokens(self, tokens: List[int]):\n        if self.language in {\"zh\", \"ja\", \"th\", \"lo\", \"my\", \"yue\"}:\n            # These languages don't typically use spaces, so it is difficult to split words"
        },
        {
            "comment": "This code splits tokens into words by checking if they can be decoded as valid unicode points. If a token can't be decoded, it considers the preceding word complete and moves on to the next word. It returns both the decoded text (words) and the original token list (word_tokens).",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":279-308",
            "content": "            # without morpheme analysis. Here, we instead split words at any\n            # position where the tokens are decoded as valid unicode points\n            return self.split_tokens_on_unicode(tokens)\n        return self.split_tokens_on_spaces(tokens)\n    def split_tokens_on_unicode(self, tokens: List[int]):\n        decoded_full = self.decode_with_timestamps(tokens)\n        replacement_char = \"\\ufffd\"\n        words = []\n        word_tokens = []\n        current_tokens = []\n        unicode_offset = 0\n        for token in tokens:\n            current_tokens.append(token)\n            decoded = self.decode_with_timestamps(current_tokens)\n            if (\n                replacement_char not in decoded\n                or decoded_full[unicode_offset + decoded.index(replacement_char)]\n                == replacement_char\n            ):\n                words.append(decoded)\n                word_tokens.append(current_tokens)\n                current_tokens = []\n                unicode_offset += len(decoded)\n        return words, word_tokens"
        },
        {
            "comment": "This code splits tokens into subwords, and then further splits them based on special characters, words with spaces, or punctuation. It returns a list of words and corresponding token lists. The `get_encoding` function loads vocabulary information from a file and returns a function for encoding strings to tokens based on the specified name and number of languages.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":310-334",
            "content": "    def split_tokens_on_spaces(self, tokens: List[int]):\n        subwords, subword_tokens_list = self.split_tokens_on_unicode(tokens)\n        words = []\n        word_tokens = []\n        for subword, subword_tokens in zip(subwords, subword_tokens_list):\n            special = subword_tokens[0] >= self.eot\n            with_space = subword.startswith(\" \")\n            punctuation = subword.strip() in string.punctuation\n            if special or with_space or punctuation or len(words) == 0:\n                words.append(subword)\n                word_tokens.append(subword_tokens)\n            else:\n                words[-1] = words[-1] + subword\n                word_tokens[-1].extend(subword_tokens)\n        return words, word_tokens\n@lru_cache(maxsize=None)\ndef get_encoding(name: str = \"gpt2\", num_languages: int = 99):\n    vocab_path = os.path.join(os.path.dirname(__file__), \"assets\", f\"{name}.tiktoken\")\n    ranks = {\n        base64.b64decode(token): int(rank)\n        for token, rank in (line.split() for line in open(vocab_path) if line)"
        },
        {
            "comment": "This code defines a function that returns a tokenizer object based on specified parameters such as multilingual, num_languages, and language. The returned tokenizer is implemented using the tiktoken Encoding class and includes special tokens for tasks like translation and transcribe. The code also handles end-of-text and start-of-transcript markers, different languages, and specific time intervals. It creates a list of special tokens and assigns their corresponding ranks in the mergeable_ranks parameter. Additionally, it caches the function to improve performance by using the @lru_cache decorator.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":335-373",
            "content": "    }\n    n_vocab = len(ranks)\n    special_tokens = {}\n    specials = [\n        \"<|endoftext|>\",\n        \"<|startoftranscript|>\",\n        *[f\"<|{lang}|>\" for lang in list(LANGUAGES.keys())[:num_languages]],\n        \"<|translate|>\",\n        \"<|transcribe|>\",\n        \"<|startoflm|>\",\n        \"<|startofprev|>\",\n        \"<|nospeech|>\",\n        \"<|notimestamps|>\",\n        *[f\"<|{i * 0.02:.2f}|>\" for i in range(1501)],\n    ]\n    for token in specials:\n        special_tokens[token] = n_vocab\n        n_vocab += 1\n    return tiktoken.Encoding(\n        name=os.path.basename(vocab_path),\n        explicit_n_vocab=n_vocab,\n        pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n        mergeable_ranks=ranks,\n        special_tokens=special_tokens,\n    )\n@lru_cache(maxsize=None)\ndef get_tokenizer(\n    multilingual: bool,\n    *,\n    num_languages: int = 99,\n    language: Optional[str] = None,\n    task: Optional[str] = None,  # Literal[\"transcribe\", \"translate\", None]\n) -> Tokenizer:\n    if language is not None:"
        },
        {
            "comment": "This code checks the given language and initializes a tokenizer based on its properties. If the language is not supported, it raises an error. The encoding name and task are determined based on whether the input is multilingual or not. Then, it returns a tokenizer object with the specified parameters.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/tokenizer.py\":374-394",
            "content": "        language = language.lower()\n        if language not in LANGUAGES:\n            if language in TO_LANGUAGE_CODE:\n                language = TO_LANGUAGE_CODE[language]\n            else:\n                raise ValueError(f\"Unsupported language: {language}\")\n    if multilingual:\n        encoding_name = \"multilingual\"\n        language = language or \"en\"\n        task = task or \"transcribe\"\n    else:\n        encoding_name = \"gpt2\"\n        language = None\n        task = None\n    encoding = get_encoding(name=encoding_name, num_languages=num_languages)\n    return Tokenizer(\n        encoding=encoding, num_languages=num_languages, language=language, task=task\n    )"
        }
    ]
}