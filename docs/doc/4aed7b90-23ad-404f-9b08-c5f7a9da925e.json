{
    "summary": "The code performs package installation, handles tar files and audio/transcriptions, trains language models using Whisper methods for transcription and translation, stores results in lists, generates DataFrame output, ensures font availability, and accurately displays text across languages. It processes first 10 dataset examples to generate logits, attention weights, and aligns audio signal with tokens, displaying word-level timestamps and segmented words using a notebook.",
    "details": [
        {
            "comment": "The code is installing the Whisper package and loading the Fleurs dataset. It is written in Python, requires TensorFlow, TorchAudio, and Scipy libraries, and uses widgets from ipywidgets library for user interaction. The language of the Fleur dataset can be selected by the user.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":0-50",
            "content": "#!/usr/bin/env python\n# coding: utf-8\n# # Installing Whisper\n# \n# The commands below will install the Python packages needed to use Whisper models and evaluate the transcription results.\n# In[1]:\nget_ipython().system(' pip install git+https://github.com/openai/whisper.git')\n# In[2]:\nimport io\nimport os\nimport numpy as np\ntry:\n    import tensorflow  # required in Colab to avoid protobuf compatibility issues\nexcept ImportError:\n    pass\nimport torch\nimport pandas as pd\nimport urllib\nimport tarfile\nimport whisper\nimport torchaudio\nfrom scipy.io import wavfile\nfrom tqdm.notebook import tqdm\npd.options.display.max_rows = 100\npd.options.display.max_colwidth = 1000\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# # Loading the Fleurs dataset\n# \n# Select the language of the Fleur dataset to download. Please note that the transcription and translation performance varies widely depending on the language. Appendix D.2 in the paper contains the performance breakdown by language.\n# In[3]:\nimport ipywidgets as widgets\nla"
        },
        {
            "comment": "This code defines a dictionary mapping language codes to their respective names.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":50-50",
            "content": "nguages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\""
        },
        {
            "comment": "The code creates a dropdown menu with language options, allows the user to select a language, and asserts that a language is selected before printing the chosen language.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":50-68",
            "content": ": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\nselection = widgets.Dropdown(\n    options=[(\"Select language\", None), (\"----------\", None)] + sorted([(f\"{v} ({k})\", k) for k, v in languages.items()]),\n    value=\"ko_kr\",\n    description='Language:',\n    disabled=False,\n)\nselection\n# In[4]:\nlang = selection.value\nlanguage = languages[lang]\nassert lang is not None, \"Please select a language\"\nprint(f\"Selected language: {language} ({lang})\")"
        },
        {
            "comment": "This function `download` takes a URL and target path as parameters, downloads the file from the given URL to the specified target path.\nThe class `Fleurs` is a wrapper for the Fleurs dataset that allows for data subsampling and supports different languages. It initializes with language, split (train or test), subsample rate, and device.\nIt creates a download link and a target path for the file, ensures the necessary directory exists, and if the file doesn't exist, it calls the `download` function to download it.\nThe class also initializes an empty dictionary `all_audio` to store the dataset audios, opens the tar archive using `tarfile.open`, and proceeds with further operations.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":71-99",
            "content": "# In[5]:\ndef download(url: str, target_path: str):\n    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n                output.write(buffer)\n                loop.update(len(buffer))\nclass Fleurs(torch.utils.data.Dataset):\n    \"\"\"\n    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n    \"\"\"\n    def __init__(self, lang, split=\"test\", subsample_rate=1, device=DEVICE):\n        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n        if not os.path.exists(tar_path):\n            download(url, tar_path)\n        all_audio = {}\n        with tarfile.open(tar_path, \"r:gz\") as tar:"
        },
        {
            "comment": "This code reads data from a tar file, extracts audio files and transcriptions, and creates a dataset for language model training.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":100-127",
            "content": "            for member in tar.getmembers():\n                name = member.name\n                if name.endswith(f\"{split}.tsv\"):\n                    labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n                if f\"/{split}/\" in name and name.endswith(\".wav\"):\n                    audio_bytes = tar.extractfile(member).read()\n                    all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]                    \n        self.labels = labels.to_dict(\"records\")[::subsample_rate]\n        self.all_audio = all_audio\n        self.device = device\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, item):\n        record = self.labels[item]\n        audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n        text = record[\"transcription\"]\n        return (audio, text)\n# In[6]:\ndataset = Fleurs(lang, subsample_rate=10)  # subsample 10% of the dataset for a quick demo"
        },
        {
            "comment": "Loading a medium Whisper model and printing its properties, transcribing and translating audio files from the dataset using multilingual or English-only options, storing transcriptions and translations in lists, creating a pandas DataFrame with the results.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":130-175",
            "content": "# # Running inference on the dataset using a medium Whisper model\n# \n# The following will take a few minutes to transcribe and translate utterances in the dataset.\n# In[7]:\nmodel = whisper.load_model(\"medium\")\nprint(\n    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n)\n# In[8]:\noptions = dict(language=language, beam_size=5, best_of=5)\ntranscribe_options = dict(task=\"transcribe\", **options)\ntranslate_options = dict(task=\"translate\", **options)\n# In[9]:\nreferences = []\ntranscriptions = []\ntranslations = []\nfor audio, text in tqdm(dataset):\n    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n    translation = model.transcribe(audio, **translate_options)[\"text\"]\n    transcriptions.append(transcription)\n    translations.append(translation)\n    references.append(text)\n# In[10]:\ndata = pd.DataFrame(dict(reference=references, transcription=transcriptions, translation=translations))\ndata\n# # Word-level timestamps using attention weights"
        },
        {
            "comment": "The code is downloading a repackaged version of the Noto Sans font, likely for use in displaying text. This ensures that the text can be displayed correctly across different languages and character sets.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":176-217",
            "content": "# \n# Below, we use the cross-attention weights to determine more granular, word-level timestamps. It uses a set of heuristics and dynamic time warping (DTW) to find the alignment between the audio and the transcript.\n# In[11]:\nget_ipython().system(' pip install dtw-python')\n# In[12]:\nimport string\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport matplotlib.ticker as ticker\nfrom IPython.display import display, HTML\nfrom whisper.tokenizer import get_tokenizer\nfrom dtw import dtw\nfrom scipy.ndimage import median_filter\nget_ipython().run_line_magic('matplotlib', 'inline')\nget_ipython().run_line_magic('config', 'InlineBackend.figure_format = \"retina\"')\n# In[13]:\nAUDIO_SAMPLES_PER_TOKEN = whisper.audio.HOP_LENGTH * 2\nAUDIO_TIME_PER_TOKEN = AUDIO_SAMPLES_PER_TOKEN / whisper.audio.SAMPLE_RATE\nmedfilt_width = 7\nqk_scale = 1.0\ntokenizer = get_tokenizer(model.is_multilingual, language=languages[lang])\n# In[14]:\n# This part downloads a repackaged version of the Noto Sans font (either CJK or non-CJK)"
        },
        {
            "comment": "This code handles text rendering for various languages in Matplotlib figures and defines two functions for splitting tokens into words based on unicode or spaces. It downloads the necessary font if it doesn't exist, and then returns the words and corresponding token lists from a given set of tokens.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":218-260",
            "content": "# to render various languages in Matplotlib figures.\nif languages[lang] in {\"Chinese\", \"Japanese\", \"Korean\"}:\n    font = \"GoNotoCJKCore.ttf\"\nelse:\n    font = \"GoNotoCurrent.ttf\"\nfont_release = \"https://github.com/satbyy/go-noto-universal/releases/download/v5.2\"\nif not os.path.exists(font):\n    download(f\"{font_release}/{font}\", font)\nprop = fm.FontProperties(fname=font)\nprops = {'fontproperties': prop}\n# In[15]:\ndef split_tokens_on_unicode(tokens: torch.Tensor):\n    words = []\n    word_tokens = []\n    current_tokens = []\n    for token in tokens.tolist():\n        current_tokens.append(token)\n        decoded = tokenizer.decode_with_timestamps(current_tokens)\n        if \"\\ufffd\" not in decoded:\n            words.append(decoded)\n            word_tokens.append(current_tokens)\n            current_tokens = []\n    return words, word_tokens\n# In[16]:\ndef split_tokens_on_spaces(tokens: torch.Tensor):\n    subwords, subword_tokens_list = split_tokens_on_unicode(tokens)\n    words = []\n    word_tokens = []\n    for subword, subword_tokens in zip(subwords, subword_tokens_list):"
        },
        {
            "comment": "Code is splitting words in different languages and setting up hooks on cross-attention layers for later use.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":261-293",
            "content": "        special = subword_tokens[0] >= tokenizer.eot\n        with_space = subword.startswith(\" \")\n        punctuation = subword.strip() in string.punctuation\n        if special or with_space or punctuation:\n            words.append(subword)\n            word_tokens.append(subword_tokens)\n        else:\n            words[-1] = words[-1] + subword\n            word_tokens[-1].extend(subword_tokens)\n    return words, word_tokens\n# In[17]:\nif languages[lang] in {\"Chinese\", \"Japanese\", \"Thai\", \"Lao\", \"Myanmar\"}:\n    # These languages don't typically use spaces, so it is difficult to split words\n    # without morpheme analysis. Here, we instead split words at any\n    # position where the tokens are decoded as valid unicode points\n    split_tokens = split_tokens_on_unicode\nelse:\n    split_tokens = split_tokens_on_spaces\n# In[18]:\n# install hooks on the cross attention layers to retrieve the attention weights\nQKs = [None] * model.dims.n_text_layer\nfor i, block in enumerate(model.decoder.blocks):\n    block.cross_attn.register_forward_hook("
        },
        {
            "comment": "Iterates through the first 10 examples in the dataset, calculating and printing transcriptions.\n- Audio and label pair for each example\n- Calculates duration of audio\n- Creates mel spectrogram from audio using Whisper's methods\n- Encodes transcription into tokens using tokenizer\n- Applies model to generate logits\n- Extracts weights from QKs matrix, applies median filtering and normalization",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":294-324",
            "content": "        lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1])\n    )\n# In[19]:\n# for the first 10 examples in the dataset\nfor (audio, label), transcription in zip(dataset, transcriptions[:10]):\n    print(transcription)\n    duration = len(audio)\n    mel = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio)).cuda()\n    tokens = torch.tensor(\n        [\n            *tokenizer.sot_sequence,\n            tokenizer.timestamp_begin,\n        ] + tokenizer.encode(transcription) + [\n            tokenizer.timestamp_begin + duration // AUDIO_SAMPLES_PER_TOKEN,\n            tokenizer.eot,\n        ]\n    ).cuda()\n    with torch.no_grad():\n        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))\n    weights = torch.cat(QKs)  # layers * heads * tokens * frames    \n    weights = weights[:, :, :, : duration // AUDIO_SAMPLES_PER_TOKEN].cpu()\n    weights = median_filter(weights, (1, 1, 1, medfilt_width))\n    weights = torch.tensor(weights * qk_scale).softmax(dim=-1)\n    w = weights / weights.norm(dim=-2, keepdim=True)"
        },
        {
            "comment": "This code is used to display the attention weights and alignment between audio signals and tokens. It calculates the mean of the last 6 rows from 'w', then uses dynamic time warping (DTW) for alignment, finds jumps in the alignment, computes jump times, splits tokens into words and word tokens, and finally plots the matrix with attention weights and alignment using matplotlib.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":325-355",
            "content": "    matrix = w[-6:].mean(axis=(0, 1))\n    alignment = dtw(-matrix.double().numpy())\n    jumps = np.pad(np.diff(alignment.index1s), (1, 0), constant_values=1).astype(bool)\n    jump_times = alignment.index2s[jumps] * AUDIO_TIME_PER_TOKEN\n    words, word_tokens = split_tokens(tokens)\n    # display the normalized attention weights and the alignment\n    plt.figure(figsize=(8, 8))\n    plt.imshow(matrix, aspect=\"auto\")\n    plt.plot(alignment.index2s, alignment.index1s, color=\"red\")\n    xticks = np.arange(0, matrix.shape[1], 1 / AUDIO_TIME_PER_TOKEN)\n    xticklabels = (xticks * AUDIO_TIME_PER_TOKEN).round().astype(np.int32) \n    plt.xticks(xticks, xticklabels)\n    plt.xlabel(\"Time (s)\")\n    # display tokens and words as tick labels\n    ylims = plt.gca().get_ylim()\n    ax = plt.gca()\n    ax.tick_params('both', length=0, width=0, which='minor', pad=6)\n    ax.yaxis.set_ticks_position(\"left\")\n    ax.yaxis.set_label_position(\"left\")\n    ax.invert_yaxis()\n    ax.set_ylim(ylims)\n    major_ticks = [-0.5]\n    minor_ticks = []"
        },
        {
            "comment": "This code generates a plot of word-level timestamps and displays the word-level timestamps in a table. It iterates over words and their corresponding token values, appends minor and major tick locations to the axes, sets the minor and major locators and formatters, sets y-ticks, removes the formatter from major ticks, adjusts the font properties of the tick labels, displays the y-axis label, shows the plot, and finally creates a table with word, begin time, and end time values for each segmented word.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":356-385",
            "content": "    current_y = 0\n    for word, word_token in zip(words, word_tokens):\n        minor_ticks.append(current_y + len(word_token) / 2 - 0.5)\n        current_y += len(word_token)\n        major_ticks.append(current_y - 0.5)\n    ax.yaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n    ax.yaxis.set_minor_formatter(ticker.FixedFormatter(words))\n    ax.set_yticks(major_ticks)\n    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n    for label in ax.get_yminorticklabels():\n        label.set_fontproperties(prop)\n    plt.ylabel(\"Words\")\n    plt.show()\n    # display the word-level timestamps in a table\n    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n    begin_times = jump_times[word_boundaries[:-1]]\n    end_times = jump_times[word_boundaries[1:]]\n    data = [\n        dict(word=word, begin=begin, end=end)\n        for word, begin, end in zip(words[:-1], begin_times, end_times)\n        if not word.startswith(\"<|\") and word.strip() not in \".,!?\u3001\u3002\"\n    ]\n    display(pd.DataFrame(data))"
        },
        {
            "comment": "Creates a horizontal line for separation in the notebook",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/notebooks/Multilingual_ASR.py\":386-386",
            "content": "    display(HTML(\"<hr>\"))"
        }
    ]
}