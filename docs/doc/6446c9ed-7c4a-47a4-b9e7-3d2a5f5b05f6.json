{
    "summary": "The code utilizes a Whisper model for audio language detection, with features like fp16 calculations, diverse data output, and speech recognition classes. It also ensures efficient processing by preventing infinite looping, incorporating functions for audio processing, language identification, tokenization, batch processing, and optional updates.",
    "details": [
        {
            "comment": "The code is defining a function `detect_language` that takes in an audio mel spectrogram, a pre-trained Whisper model and optionally a tokenizer. It detects the spoken language in the audio and returns the list of detected languages as strings along with their corresponding ids and probability distribution over all language tokens. The function is decorated with `@torch.no_grad()` to avoid unnecessary memory allocation during inference.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":0-29",
            "content": "from dataclasses import dataclass, field, replace\nfrom typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.distributions import Categorical\nfrom .audio import CHUNK_LENGTH\nfrom .tokenizer import Tokenizer, get_tokenizer\nfrom .utils import compression_ratio\nif TYPE_CHECKING:\n    from .model import Whisper\n@torch.no_grad()\ndef detect_language(\n    model: \"Whisper\", mel: Tensor, tokenizer: Tokenizer = None\n) -> Tuple[Tensor, List[dict]]:\n    \"\"\"\n    Detect the spoken language in the audio, and return them as list of strings, along with the ids\n    of the most probable language tokens and the probability distribution over all language tokens.\n    This is performed outside the main decode loop in order to not interfere with kv-caching.\n    Returns\n    -------\n    language_tokens : Tensor, shape = (n_audio,)\n        ids of the most probable language tokens, which appears after the startoftranscript token."
        },
        {
            "comment": "This code prepares input for language identification by either using a pre-existing tokenizer or creating one based on the model's configuration. It then performs an encoder forward pass and a forward pass using a single token (startoftranscript) to generate logits, which represent the detected languages while suppressing non-language tokens.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":30-58",
            "content": "    language_probs : List[Dict[str, float]], length = n_audio\n        list of dictionaries containing the probability distribution over all languages.\n    \"\"\"\n    if tokenizer is None:\n        tokenizer = get_tokenizer(\n            model.is_multilingual, num_languages=model.num_languages\n        )\n    if (\n        tokenizer.language is None\n        or tokenizer.language_token not in tokenizer.sot_sequence\n    ):\n        raise ValueError(\n            \"This model doesn't have language tokens so it can't perform lang id\"\n        )\n    single = mel.ndim == 2\n    if single:\n        mel = mel.unsqueeze(0)\n    # skip encoder forward pass if already-encoded audio features were given\n    if mel.shape[-2:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):\n        mel = model.encoder(mel)\n    # forward pass using a single token, startoftranscript\n    n_audio = mel.shape[0]\n    x = torch.tensor([[tokenizer.sot]] * n_audio).to(mel.device)  # [n_audio, 1]\n    logits = model.logits(x, mel)[:, 0]\n    # collect detected languages; suppress all non-language tokens"
        },
        {
            "comment": "This code is for decoding audio in a model. It takes logits, tokenizer, and other parameters as input to predict the language tokens and their probabilities. The code also includes options for task type (transcribe or translate), language detection, temperature control for sampling, and best-of options. The function returns language tokens and their probabilities.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":59-90",
            "content": "    mask = torch.ones(logits.shape[-1], dtype=torch.bool)\n    mask[list(tokenizer.all_language_tokens)] = False\n    logits[:, mask] = -np.inf\n    language_tokens = logits.argmax(dim=-1)\n    language_token_probs = logits.softmax(dim=-1).cpu()\n    language_probs = [\n        {\n            c: language_token_probs[i, j].item()\n            for j, c in zip(tokenizer.all_language_tokens, tokenizer.all_language_codes)\n        }\n        for i in range(n_audio)\n    ]\n    if single:\n        language_tokens = language_tokens[0]\n        language_probs = language_probs[0]\n    return language_tokens, language_probs\n@dataclass(frozen=True)\nclass DecodingOptions:\n    # whether to perform X->X \"transcribe\" or X->English \"translate\"\n    task: str = \"transcribe\"\n    # language that the audio is in; uses detected language if None\n    language: Optional[str] = None\n    # sampling-related options\n    temperature: float = 0.0\n    sample_len: Optional[int] = None  # maximum number of tokens to sample\n    best_of: Optional[int] = None  # number of independent sample trajectories, if t > 0"
        },
        {
            "comment": "This code defines a class with various parameters for controlling the decoding process of a speech-to-text model. The beam_size, patience, and length_penalty parameters control the beam search algorithm used in the decoding process. The prompt and prefix parameters allow feeding additional context to the model. The suppress_tokens parameter allows suppressing certain token ids or symbol sets. The suppress_blank parameter controls whether to suppress blank outputs. Lastly, the without_timestamps parameter can be used to sample text tokens only.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":91-109",
            "content": "    beam_size: Optional[int] = None  # number of beams in beam search, if t == 0\n    patience: Optional[float] = None  # patience in beam search (arxiv:2204.05424)\n    # \"alpha\" in Google NMT, or None for length norm, when ranking generations\n    # to select which to return among the beams or best-of-N samples\n    length_penalty: Optional[float] = None\n    # text or tokens to feed as the prompt or the prefix; for more info:\n    # https://github.com/openai/whisper/discussions/117#discussioncomment-3727051\n    prompt: Optional[Union[str, List[int]]] = None  # for the previous context\n    prefix: Optional[Union[str, List[int]]] = None  # to prefix the current context\n    # list of tokens ids (or comma-separated token ids) to suppress\n    # \"-1\" will suppress a set of symbols as defined in `tokenizer.non_speech_tokens()`\n    suppress_tokens: Optional[Union[str, Iterable[int]]] = \"-1\"\n    suppress_blank: bool = True  # this will suppress blank outputs\n    # timestamp sampling options\n    without_timestamps: bool = False  # use <|notimestamps|> to sample text tokens only"
        },
        {
            "comment": "This code defines classes and methods for a whisper inference module. It has properties such as maximum initial timestamp, uses fp16 calculations, and returns results like audio features, language, token lists, text, average log probabilities, no speech probabilities, temperatures, and compression ratios. The class also includes functions to perform forward passes, update key-value caches, and clean up resources after decoding is complete.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":110-144",
            "content": "    max_initial_timestamp: Optional[float] = 1.0\n    # implementation details\n    fp16: bool = True  # use fp16 for most of the calculation\n@dataclass(frozen=True)\nclass DecodingResult:\n    audio_features: Tensor\n    language: str\n    language_probs: Optional[Dict[str, float]] = None\n    tokens: List[int] = field(default_factory=list)\n    text: str = \"\"\n    avg_logprob: float = np.nan\n    no_speech_prob: float = np.nan\n    temperature: float = np.nan\n    compression_ratio: float = np.nan\nclass Inference:\n    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n        \"\"\"Perform a forward pass on the decoder and return per-token logits\"\"\"\n        raise NotImplementedError\n    def rearrange_kv_cache(self, source_indices) -> None:\n        \"\"\"Update the key-value cache according to the updated beams\"\"\"\n        raise NotImplementedError\n    def cleanup_caching(self) -> None:\n        \"\"\"Clean up any resources or hooks after decoding is finished\"\"\"\n        pass\nclass PyTorchInference(Inference):\n    def __init__(self, model: \"Whisper\", initial_token_length: int):"
        },
        {
            "comment": "This code initializes a Whisper model and sets up key-value cache hooks for caching attention keys and values during inference. The logits function calculates the logits, using the cache if it exists. The cleanup_caching function removes all installed hooks and resets the cache, and the rearrange_kv_cache function can be used to reorder the cached key-value pairs based on given source indices.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":145-172",
            "content": "        self.model: \"Whisper\" = model\n        self.initial_token_length = initial_token_length\n        self.kv_cache = {}\n        self.hooks = []\n        key_modules = [block.attn.key for block in self.model.decoder.blocks]\n        value_modules = [block.attn.value for block in self.model.decoder.blocks]\n        self.kv_modules = key_modules + value_modules\n    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n        if not self.kv_cache:\n            self.kv_cache, self.hooks = self.model.install_kv_cache_hooks()\n        if tokens.shape[-1] > self.initial_token_length:\n            # only need to use the last token except in the first forward pass\n            tokens = tokens[:, -1:]\n        return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n    def cleanup_caching(self):\n        for hook in self.hooks:\n            hook.remove()\n        self.kv_cache = {}\n        self.hooks = []\n    def rearrange_kv_cache(self, source_indices):\n        if source_indices != list(range(len(source_indices))):"
        },
        {
            "comment": "The code defines a class `SequenceRanker` with a method `rank()` that takes a list of groups of samples and their cumulative log probabilities as input, and returns the indices of the samples in each group to select as the final result. The code also includes a subclass `MaximumLikelihoodRanker` that extends `SequenceRanker`, allowing the selection of samples with the highest log probabilities, penalized using either simple length normalization or Google NMT paper's length penalty.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":173-200",
            "content": "            for module in self.kv_modules:\n                # update the key/value cache to contain the selected sequences\n                self.kv_cache[module] = self.kv_cache[module][source_indices].detach()\nclass SequenceRanker:\n    def rank(\n        self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]\n    ) -> List[int]:\n        \"\"\"\n        Given a list of groups of samples and their cumulative log probabilities,\n        return the indices of the samples in each group to select as the final result\n        \"\"\"\n        raise NotImplementedError\nclass MaximumLikelihoodRanker(SequenceRanker):\n    \"\"\"\n    Select the sample with the highest log probabilities, penalized using either\n    a simple length normalization or Google NMT paper's length penalty\n    \"\"\"\n    def __init__(self, length_penalty: Optional[float]):\n        self.length_penalty = length_penalty\n    def rank(self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]):\n        def scores(logprobs, lengths):\n            result = []"
        },
        {
            "comment": "This code defines a class `TokenDecoder` that is responsible for decoding token sequences in sequence-to-sequence tasks. The `reset` function initializes any stateful variables needed for decoding a new sequence. The `update` function takes in `tokens`, `logits`, and `sum_logprobs` as input, and returns the next token and whether it is an end-of-sequence token.\n\nThe `reset` function: \n- Initializes any stateful variables needed for decoding a new sequence.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":201-227",
            "content": "            for logprob, length in zip(logprobs, lengths):\n                if self.length_penalty is None:\n                    penalty = length\n                else:\n                    # from the Google NMT paper\n                    penalty = ((5 + length) / 6) ** self.length_penalty\n                result.append(logprob / penalty)\n            return result\n        # get the sequence with the highest score\n        lengths = [[len(t) for t in s] for s in tokens]\n        return [np.argmax(scores(p, l)) for p, l in zip(sum_logprobs, lengths)]\nclass TokenDecoder:\n    def reset(self):\n        \"\"\"Initialize any stateful variables for decoding a new sequence\"\"\"\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        \"\"\"Specify how to select the next token, based on the current trace and logits\n        Parameters\n        ----------\n        tokens : Tensor, shape = (n_batch, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence tokens"
        },
        {
            "comment": "This code defines a class that seems to be related to speech recognition. It has two methods: 'decode' and 'finalize'. The 'decode' method takes logits and cumulative log probabilities as input, returns the tokens and a boolean value indicating if all sequences have reached the end of text. The 'finalize' method takes tokens and cumulative log probabilities, and returns final candidate sequences.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":229-257",
            "content": "        logits : Tensor, shape = (n_batch, vocab_size)\n            per-token logits of the probability distribution at the current step\n        sum_logprobs : Tensor, shape = (n_batch)\n            cumulative log probabilities for each sequence\n        Returns\n        -------\n        tokens : Tensor, shape = (n_batch, current_sequence_length + 1)\n            the tokens, appended with the selected next token\n        completed : bool\n            True if all sequences has reached the end of text\n        \"\"\"\n        raise NotImplementedError\n    def finalize(\n        self, tokens: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Sequence[Sequence[Tensor]], List[List[float]]]:\n        \"\"\"Finalize search and return the final candidate sequences\n        Parameters\n        ----------\n        tokens : Tensor, shape = (n_audio, n_group, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence\n        sum_logprobs : Tensor, shape = (n_audio, n_group)\n            cumulative log probabilities for each sequence"
        },
        {
            "comment": "This code defines a GreedyDecoder class that takes temperature and eot (end of text) parameters. It implements an update() function that given tokens, logits, and sum_logprobs, returns updated tokens and a boolean indicating whether the end of text has been reached. If the temperature is 0, it selects the most probable token using argmax. Otherwise, it uses the softmax distribution to sample a token at the specified temperature. It also updates the logprobs and sum_logprobs for each token.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":259-288",
            "content": "        Returns\n        -------\n        tokens : Sequence[Sequence[Tensor]], length = n_audio\n            sequence of Tensors containing candidate token sequences, for each audio input\n        sum_logprobs : List[List[float]], length = n_audio\n            sequence of cumulative log probabilities corresponding to the above\n        \"\"\"\n        raise NotImplementedError\nclass GreedyDecoder(TokenDecoder):\n    def __init__(self, temperature: float, eot: int):\n        self.temperature = temperature\n        self.eot = eot\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        if self.temperature == 0:\n            next_tokens = logits.argmax(dim=-1)\n        else:\n            next_tokens = Categorical(logits=logits / self.temperature).sample()\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n        sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n        next_tokens[tokens[:, -1] == self.eot] = self.eot"
        },
        {
            "comment": "The code defines a BeamSearchDecoder class for decoding sequences using beam search algorithm. It initializes with a beam size, EOT token, inference type, and patience value. It also has methods for resetting and updating the decoder during the decoding process.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":289-323",
            "content": "        tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n        completed = (tokens[:, -1] == self.eot).all()\n        return tokens, completed\n    def finalize(self, tokens: Tensor, sum_logprobs: Tensor):\n        # make sure each sequence has at least one EOT token at the end\n        tokens = F.pad(tokens, (0, 1), value=self.eot)\n        return tokens, sum_logprobs.tolist()\nclass BeamSearchDecoder(TokenDecoder):\n    def __init__(\n        self,\n        beam_size: int,\n        eot: int,\n        inference: Inference,\n        patience: Optional[float] = None,\n    ):\n        self.beam_size = beam_size\n        self.eot = eot\n        self.inference = inference\n        self.patience = patience or 1.0\n        self.max_candidates: int = round(beam_size * self.patience)\n        self.finished_sequences = None\n        assert (\n            self.max_candidates > 0\n        ), f\"Invalid beam size ({beam_size}) or patience ({patience})\"\n    def reset(self):\n        self.finished_sequences = None\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor"
        },
        {
            "comment": "This function divides the input tokens into multiple audio sequences based on the beam size. It checks if the number of tokens is a multiple of the beam size, and raises an error if not. It then calculates cumulative log probabilities for possible candidates in each audio sequence.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":324-344",
            "content": "    ) -> Tuple[Tensor, bool]:\n        if tokens.shape[0] % self.beam_size != 0:\n            raise ValueError(f\"{tokens.shape}[0] % {self.beam_size} != 0\")\n        n_audio = tokens.shape[0] // self.beam_size\n        if self.finished_sequences is None:  # for the first update\n            self.finished_sequences = [{} for _ in range(n_audio)]\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        next_tokens, source_indices, finished_sequences = [], [], []\n        for i in range(n_audio):\n            scores, sources, finished = {}, {}, {}\n            # STEP 1: calculate the cumulative log probabilities for possible candidates\n            for j in range(self.beam_size):\n                idx = i * self.beam_size + j\n                prefix = tokens[idx].tolist()\n                for logprob, token in zip(*logprobs[idx].topk(self.beam_size + 1)):\n                    new_logprob = (sum_logprobs[idx] + logprob).item()\n                    sequence = tuple(prefix + [token.item()])\n                    scores[sequence] = new_logprob"
        },
        {
            "comment": "This code is ranking and selecting the top N sequences from a list of candidate sequences based on their scores. It keeps track of finished sequences and updates the inference cache with new data.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":345-369",
            "content": "                    sources[sequence] = idx\n            # STEP 2: rank the candidates and keep the top beam_size sequences for each audio\n            saved = 0\n            for sequence in sorted(scores, key=scores.get, reverse=True):\n                if sequence[-1] == self.eot:\n                    finished[sequence] = scores[sequence]\n                else:\n                    sum_logprobs[len(next_tokens)] = scores[sequence]\n                    next_tokens.append(sequence)\n                    source_indices.append(sources[sequence])\n                    saved += 1\n                    if saved == self.beam_size:\n                        break\n            finished_sequences.append(finished)\n        tokens = torch.tensor(next_tokens, device=tokens.device)\n        self.inference.rearrange_kv_cache(source_indices)\n        # add newly finished sequences to self.finished_sequences\n        assert len(self.finished_sequences) == len(finished_sequences)\n        for previously_finished, newly_finished in zip(\n            self.finished_sequences, finished_sequences"
        },
        {
            "comment": "Code adds more finished sequences to the list if there are not enough. It also marks all sequences as completed if they have enough samples.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":370-391",
            "content": "        ):\n            for seq in sorted(newly_finished, key=newly_finished.get, reverse=True):\n                if len(previously_finished) >= self.max_candidates:\n                    break  # the candidate list is full\n                previously_finished[seq] = newly_finished[seq]\n        # mark as completed if all audio has enough number of samples\n        completed = all(\n            len(sequences) >= self.max_candidates\n            for sequences in self.finished_sequences\n        )\n        return tokens, completed\n    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor):\n        # collect all finished sequences, including patience, and add unfinished ones if not enough\n        sum_logprobs = sum_logprobs.cpu()\n        for i, sequences in enumerate(self.finished_sequences):\n            if (\n                len(sequences) < self.beam_size\n            ):  # when not enough sequences are finished\n                for j in list(np.argsort(sum_logprobs[i]))[::-1]:\n                    sequence = preceding_tokens[i, j].tolist() + [self.eot]"
        },
        {
            "comment": "This code is a part of a decoding process in a deep learning model. It calculates the log probabilities of each sequence and stores them in a dictionary called \"sequences\". If the length of sequences reaches a certain threshold (beam_size), the loop breaks. Then, it creates two lists: tokens and sum_logprobs from the data stored in self.finished_sequences. Finally, it returns these two lists as output.\n\nThe LogitFilter class is an abstract base class for applying filtering or masking to logits during decoding. It has a method \"apply\" which takes logits and tokens as input but does not return anything. The SuppressBlank class inherits from LogitFilter and requires implementing the apply method according to specific requirements.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":392-422",
            "content": "                    sequences[tuple(sequence)] = sum_logprobs[i][j].item()\n                    if len(sequences) >= self.beam_size:\n                        break\n        tokens: List[List[Tensor]] = [\n            [torch.tensor(seq) for seq in sequences.keys()]\n            for sequences in self.finished_sequences\n        ]\n        sum_logprobs: List[List[float]] = [\n            list(sequences.values()) for sequences in self.finished_sequences\n        ]\n        return tokens, sum_logprobs\nclass LogitFilter:\n    def apply(self, logits: Tensor, tokens: Tensor) -> None:\n        \"\"\"Apply any filtering or masking to logits in-place\n        Parameters\n        ----------\n        logits : Tensor, shape = (n_batch, vocab_size)\n            per-token logits of the probability distribution at the current step\n        tokens : Tensor, shape = (n_batch, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence tokens\n        \"\"\"\n        raise NotImplementedError\nclass SuppressBlank(LogitFilter):"
        },
        {
            "comment": "The code contains three classes, each representing a different method of filtering logits in a machine translation model. \"SuppressTokens\" applies a negative infinity penalty to specified token indices. \"ApplyTimestampRules\" suppresses <|notimestamps|> and sets negative penalties for certain tokens based on sample_begin and max_initial_timestamp_index. \"SuppressEmptyTokens\" sets a negative penalty for empty tokens.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":423-452",
            "content": "    def __init__(self, tokenizer: Tokenizer, sample_begin: int):\n        self.tokenizer = tokenizer\n        self.sample_begin = sample_begin\n    def apply(self, logits: Tensor, tokens: Tensor):\n        if tokens.shape[1] == self.sample_begin:\n            logits[:, self.tokenizer.encode(\" \") + [self.tokenizer.eot]] = -np.inf\nclass SuppressTokens(LogitFilter):\n    def __init__(self, suppress_tokens: Sequence[int]):\n        self.suppress_tokens = list(suppress_tokens)\n    def apply(self, logits: Tensor, tokens: Tensor):\n        logits[:, self.suppress_tokens] = -np.inf\nclass ApplyTimestampRules(LogitFilter):\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        sample_begin: int,\n        max_initial_timestamp_index: Optional[int],\n    ):\n        self.tokenizer = tokenizer\n        self.sample_begin = sample_begin\n        self.max_initial_timestamp_index = max_initial_timestamp_index\n    def apply(self, logits: Tensor, tokens: Tensor):\n        # suppress <|notimestamps|> which is handled by without_timestamps"
        },
        {
            "comment": "This code masks certain logits based on whether the preceding tokens are timestamps or not. If the preceding token is a timestamp, it sets corresponding logits to -inf, preventing them from being selected during sampling.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":453-474",
            "content": "        if self.tokenizer.no_timestamps is not None:\n            logits[:, self.tokenizer.no_timestamps] = -np.inf\n        # timestamps have to appear in pairs, except directly before EOT; mask logits accordingly\n        for k in range(tokens.shape[0]):\n            sampled_tokens = tokens[k, self.sample_begin :]\n            seq = [t for t in sampled_tokens.tolist()]\n            last_was_timestamp = (\n                len(seq) >= 1 and seq[-1] >= self.tokenizer.timestamp_begin\n            )\n            penultimate_was_timestamp = (\n                len(seq) < 2 or seq[-2] >= self.tokenizer.timestamp_begin\n            )\n            if last_was_timestamp:\n                if penultimate_was_timestamp:  # has to be non-timestamp\n                    logits[k, self.tokenizer.timestamp_begin :] = -np.inf\n                else:  # cannot be normal text tokens\n                    logits[k, : self.tokenizer.eot] = -np.inf\n            timestamps = sampled_tokens[\n                sampled_tokens.ge(self.tokenizer.timestamp_begin)"
        },
        {
            "comment": "This code is ensuring that the timestamps in the decoding process don't decrease and forcing each segment to have a non-zero length, preventing infinite looping. It also suppresses generating non-timestamp tokens at the beginning of the sequence and applies the `max_initial_timestamp` option if specified.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":475-494",
            "content": "            ]\n            if timestamps.numel() > 0:\n                # timestamps shouldn't decrease; forbid timestamp tokens smaller than the last\n                # also force each segment to have a nonzero length, to prevent infinite looping\n                if last_was_timestamp and not penultimate_was_timestamp:\n                    timestamp_last = timestamps[-1]\n                else:\n                    timestamp_last = timestamps[-1] + 1\n                logits[k, self.tokenizer.timestamp_begin : timestamp_last] = -np.inf\n        if tokens.shape[1] == self.sample_begin:\n            # suppress generating non-timestamp tokens at the beginning\n            logits[:, : self.tokenizer.timestamp_begin] = -np.inf\n            # apply the `max_initial_timestamp` option\n            if self.max_initial_timestamp_index is not None:\n                last_allowed = (\n                    self.tokenizer.timestamp_begin + self.max_initial_timestamp_index\n                )\n                logits[:, last_allowed + 1 :] = -np.inf"
        },
        {
            "comment": "This code snippet is part of a machine learning model that performs speech-to-text decoding. It calculates the probability scores for each timestamp and token, then selects the timestamp with the highest log probability. The selected timestamp is used to sample the next token from the text tokens.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":496-523",
            "content": "        # if sum of probability over timestamps is above any other token, sample timestamp\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        for k in range(tokens.shape[0]):\n            timestamp_logprob = logprobs[k, self.tokenizer.timestamp_begin :].logsumexp(\n                dim=-1\n            )\n            max_text_token_logprob = logprobs[k, : self.tokenizer.timestamp_begin].max()\n            if timestamp_logprob > max_text_token_logprob:\n                logits[k, : self.tokenizer.timestamp_begin] = -np.inf\nclass DecodingTask:\n    inference: Inference\n    sequence_ranker: SequenceRanker\n    decoder: TokenDecoder\n    logit_filters: List[LogitFilter]\n    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n        self.model = model\n        language = options.language or \"en\"\n        tokenizer = get_tokenizer(\n            model.is_multilingual,\n            num_languages=model.num_languages,\n            language=language,\n            task=options.task,\n        )\n        self.tokenizer: Tokenizer = tokenizer"
        },
        {
            "comment": "This code initializes various variables for decoding, such as beam size, maximum sample length, and initial tokens. It also creates instances of PyTorchInference and MaximumLikelihoodRanker classes, which are used for inference and sequence ranking respectively.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":524-544",
            "content": "        self.options: DecodingOptions = self._verify_options(options)\n        self.n_group: int = options.beam_size or options.best_of or 1\n        self.n_ctx: int = model.dims.n_text_ctx\n        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2\n        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence\n        if self.options.without_timestamps:\n            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n        self.initial_tokens: Tuple[int] = self._get_initial_tokens()\n        self.sample_begin: int = len(self.initial_tokens)\n        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)\n        # inference: implements the forward pass through the decoder, including kv caching\n        self.inference = PyTorchInference(model, len(self.initial_tokens))\n        # sequence ranker: implements how to rank a group of sampled sequences\n        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)\n        # decoder: implements how to select the next tokens, given the autoregressive distribution"
        },
        {
            "comment": "This code initializes the decoder and logit filters for a model. The decoder is chosen based on the beam size option, with BeamSearchDecoder used if specified or GreedyDecoder otherwise. Logit filters are added based on options to suppress blank tokens or specific tokens. If timestamps are not disabled, a timestamp filter is also added.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":545-565",
            "content": "        if options.beam_size is not None:\n            self.decoder = BeamSearchDecoder(\n                options.beam_size, tokenizer.eot, self.inference, options.patience\n            )\n        else:\n            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)\n        # logit filters: applies various rules to suppress or penalize certain tokens\n        self.logit_filters = []\n        if self.options.suppress_blank:\n            self.logit_filters.append(SuppressBlank(self.tokenizer, self.sample_begin))\n        if self.options.suppress_tokens:\n            self.logit_filters.append(SuppressTokens(self._get_suppress_tokens()))\n        if not options.without_timestamps:\n            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds\n            max_initial_timestamp_index = None\n            if options.max_initial_timestamp:\n                max_initial_timestamp_index = round(\n                    self.options.max_initial_timestamp / precision\n                )\n            self.logit_filters.append("
        },
        {
            "comment": "This code defines a class with methods to initialize decoding options, verify them for compatibility, and get initial tokens. It also applies timestamp rules to tokens if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":566-589",
            "content": "                ApplyTimestampRules(\n                    tokenizer, self.sample_begin, max_initial_timestamp_index\n                )\n            )\n    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:\n        if options.beam_size is not None and options.best_of is not None:\n            raise ValueError(\"beam_size and best_of can't be given together\")\n        if options.temperature == 0:\n            if options.best_of is not None:\n                raise ValueError(\"best_of with greedy sampling (T=0) is not compatible\")\n        if options.patience is not None and options.beam_size is None:\n            raise ValueError(\"patience requires beam_size to be given\")\n        if options.length_penalty is not None and not (\n            0 <= options.length_penalty <= 1\n        ):\n            raise ValueError(\"length_penalty (alpha) should be a value between 0 and 1\")\n        return options\n    def _get_initial_tokens(self) -> Tuple[int]:\n        tokens = list(self.sot_sequence)\n        if prefix := self.options.prefix:"
        },
        {
            "comment": "Code snippet encodes prefix and prompt tokens, handles sample length, and defines suppress_tokens.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":590-618",
            "content": "            prefix_tokens = (\n                self.tokenizer.encode(\" \" + prefix.strip())\n                if isinstance(prefix, str)\n                else prefix\n            )\n            if self.sample_len is not None:\n                max_prefix_len = self.n_ctx // 2 - self.sample_len\n                prefix_tokens = prefix_tokens[-max_prefix_len:]\n            tokens = tokens + prefix_tokens\n        if prompt := self.options.prompt:\n            prompt_tokens = (\n                self.tokenizer.encode(\" \" + prompt.strip())\n                if isinstance(prompt, str)\n                else prompt\n            )\n            tokens = (\n                [self.tokenizer.sot_prev]\n                + prompt_tokens[-(self.n_ctx // 2 - 1) :]\n                + tokens\n            )\n        return tuple(tokens)\n    def _get_suppress_tokens(self) -> Tuple[int]:\n        suppress_tokens = self.options.suppress_tokens\n        if isinstance(suppress_tokens, str):\n            suppress_tokens = [int(t) for t in suppress_tokens.split(\",\")]"
        },
        {
            "comment": "This code is responsible for managing a list of tokens that represent different aspects of audio processing, such as non-speech segments, transcribing, translating, and marking start of a segment. It ensures the list is in proper format and order, then extends it with specific tokens from the tokenizer, before possibly appending a no-speech probability if available.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":620-647",
            "content": "        if -1 in suppress_tokens:\n            suppress_tokens = [t for t in suppress_tokens if t >= 0]\n            suppress_tokens.extend(self.tokenizer.non_speech_tokens)\n        elif suppress_tokens is None or len(suppress_tokens) == 0:\n            suppress_tokens = []  # interpret empty string as an empty list\n        else:\n            assert isinstance(suppress_tokens, list), \"suppress_tokens must be a list\"\n        suppress_tokens.extend(\n            [\n                self.tokenizer.transcribe,\n                self.tokenizer.translate,\n                self.tokenizer.sot,\n                self.tokenizer.sot_prev,\n                self.tokenizer.sot_lm,\n            ]\n        )\n        if self.tokenizer.no_speech is not None:\n            # no-speech probability is collected separately\n            suppress_tokens.append(self.tokenizer.no_speech)\n        return tuple(sorted(set(suppress_tokens)))\n    def _get_audio_features(self, mel: Tensor):\n        if self.options.fp16:\n            mel = mel.half()\n        if mel.shape[-2:] == ("
        },
        {
            "comment": "This code defines two functions, `_get_audio_features` and `_detect_language`. \n\n`_get_audio_features` checks if the encoded audio features are given or not. If they are given, it skips audio encoding. Otherwise, it encodes the audio using `self.model.encoder(mel)`. It also checks if the audio features have the correct dtype (either float16 or float32).\n\n`_detect_language` either retrieves the specified language from options or detects languages using `self.model.detect_language(audio_features, self.tokenizer)` when no specific language is given or task is 'lang_id'. It assigns the detected language to `languages` and calculates the language probabilities (`lang_probs`) if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":648-674",
            "content": "            self.model.dims.n_audio_ctx,\n            self.model.dims.n_audio_state,\n        ):\n            # encoded audio features are given; skip audio encoding\n            audio_features = mel\n        else:\n            audio_features = self.model.encoder(mel)\n        if audio_features.dtype != (\n            torch.float16 if self.options.fp16 else torch.float32\n        ):\n            return TypeError(\n                f\"audio_features has an incorrect dtype: {audio_features.dtype}\"\n            )\n        return audio_features\n    def _detect_language(self, audio_features: Tensor, tokens: Tensor):\n        languages = [self.options.language] * audio_features.shape[0]\n        lang_probs = None\n        if self.options.language is None or self.options.task == \"lang_id\":\n            lang_tokens, lang_probs = self.model.detect_language(\n                audio_features, self.tokenizer\n            )\n            languages = [max(probs, key=probs.get) for probs in lang_probs]\n            if self.options.language is None:"
        },
        {
            "comment": "This code defines a function that performs inference on audio features and returns the languages and language probabilities. It also contains a loop that processes each chunk of audio features, calculates logits, applies logit filters, and keeps track of no_speech_probs if there is a specified no_speech token.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":675-698",
            "content": "                tokens[:, self.sot_index + 1] = lang_tokens  # write language tokens\n        return languages, lang_probs\n    def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n        n_batch = tokens.shape[0]\n        sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n        no_speech_probs = [np.nan] * n_batch\n        try:\n            for i in range(self.sample_len):\n                logits = self.inference.logits(tokens, audio_features)\n                if (\n                    i == 0 and self.tokenizer.no_speech is not None\n                ):  # save no_speech_probs\n                    probs_at_sot = logits[:, self.sot_index].float().softmax(dim=-1)\n                    no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech].tolist()\n                # now we need to consider the logits at the last token only\n                logits = logits[:, -1]\n                # apply the logit filters, e.g. for suppressing or applying penalty to\n                for logit_filter in self.logit_filters:"
        },
        {
            "comment": "The code is performing inference for text generation or language identification using a whisper model. It starts by resetting the decoder, tokenizing input audio, and getting audio features. The code then updates tokens based on selected next tokens and applies a logit filter. If a token has been completed or if there are too many tokens, it breaks the loop. Finally, it cleans up caching before returning the results.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":699-724",
            "content": "                    logit_filter.apply(logits, tokens)\n                # expand the tokens tensor with the selected next tokens\n                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n                if completed or tokens.shape[-1] > self.n_ctx:\n                    break\n        finally:\n            self.inference.cleanup_caching()\n        return tokens, sum_logprobs, no_speech_probs\n    @torch.no_grad()\n    def run(self, mel: Tensor) -> List[DecodingResult]:\n        self.decoder.reset()\n        tokenizer: Tokenizer = self.tokenizer\n        n_audio: int = mel.shape[0]\n        audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n        tokens: Tensor = torch.tensor([self.initial_tokens]).repeat(n_audio, 1)\n        # detect language if requested, overwriting the language token\n        languages, language_probs = self._detect_language(audio_features, tokens)\n        if self.options.task == \"lang_id\":\n            return [\n                DecodingResult("
        },
        {
            "comment": "This code performs text-to-speech decoding by splitting the input audio into groups, repeating the text tensors for each group, then running a main sampling loop to generate speech tokens. The final candidates are extracted for each group and sliced up to the end of text (EOT) token.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":725-746",
            "content": "                    audio_features=features, language=language, language_probs=probs\n                )\n                for features, language, probs in zip(\n                    audio_features, languages, language_probs\n                )\n            ]\n        # repeat text tensors by the group size, for beam search or best-of-n sampling\n        tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n        # call the main sampling loop\n        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n        # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n        audio_features = audio_features[:: self.n_group]\n        no_speech_probs = no_speech_probs[:: self.n_group]\n        assert audio_features.shape[0] == len(no_speech_probs) == n_audio\n        tokens = tokens.reshape(n_audio, self.n_group, -1)\n        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n        # get the final candidates for each group, and slice between the first sampled token and EOT"
        },
        {
            "comment": "This code snippet is part of a text decoding process. It selects the top-ranked sample from each group, calculates average log probabilities, and stores results in various formats (texts, languages, tokens, audio features). The code raises an error if result lengths are inconsistent.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":747-774",
            "content": "        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n        tokens: List[List[Tensor]] = [\n            [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s]\n            for s in tokens\n        ]\n        # select the top-ranked sample in each group\n        selected = self.sequence_ranker.rank(tokens, sum_logprobs)\n        tokens: List[List[int]] = [t[i].tolist() for i, t in zip(selected, tokens)]\n        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]\n        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]\n        avg_logprobs: List[float] = [\n            lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)\n        ]\n        fields = (\n            texts,\n            languages,\n            tokens,\n            audio_features,\n            avg_logprobs,\n            no_speech_probs,\n        )\n        if len(set(map(len, fields))) != 1:\n            raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n        return ["
        },
        {
            "comment": "This code defines a function `decode` which takes in a Whisper model, Mel spectrogram(s), and optional decoding options. It performs the decoding of 30-second audio segment(s) by using the given model and mel spectrogram(s). The decoded result is returned as a DecodingResult object or a list of DecodingResult objects if multiple segments are provided.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":775-810",
            "content": "            DecodingResult(\n                audio_features=features,\n                language=language,\n                tokens=tokens,\n                text=text,\n                avg_logprob=avg_logprob,\n                no_speech_prob=no_speech_prob,\n                temperature=self.options.temperature,\n                compression_ratio=compression_ratio(text),\n            )\n            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(\n                *fields\n            )\n        ]\n@torch.no_grad()\ndef decode(\n    model: \"Whisper\",\n    mel: Tensor,\n    options: DecodingOptions = DecodingOptions(),\n    **kwargs,\n) -> Union[DecodingResult, List[DecodingResult]]:\n    \"\"\"\n    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).\n    Parameters\n    ----------\n    model: Whisper\n        the Whisper model instance\n    mel: torch.Tensor, shape = (80, 3000) or (*, 80, 3000)\n        A tensor containing the Mel spectrogram(s)\n    options: DecodingOptions\n        A dataclass that contains all necessary options for decoding 30-second segments"
        },
        {
            "comment": "This function takes a mel spectrogram and options as inputs, and uses a decoding task to perform speech recognition. If the mel spectrogram has 2 dimensions (single audio file), it is reshaped to have 3 dimensions (for batch processing). The options can be updated with additional keyword arguments. It then runs the decoding task with the model and options, returning the first result if a single audio file was input, or all results otherwise.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/decoding.py\":812-825",
            "content": "    Returns\n    -------\n    result: Union[DecodingResult, List[DecodingResult]]\n        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)\n    \"\"\"\n    if single := mel.ndim == 2:\n        mel = mel.unsqueeze(0)\n    if kwargs:\n        options = replace(options, **kwargs)\n    result = DecodingTask(model, options).run(mel)\n    return result[0] if single else result"
        }
    ]
}