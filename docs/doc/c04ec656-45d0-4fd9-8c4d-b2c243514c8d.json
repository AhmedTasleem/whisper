{
    "summary": "The code converts Wav.scp files to WAV format, preprocesses datasets like WSJ, CORAAL, and CHiME-6, provides timings and labels for The Late Show with Stephen Colbert dataset, and discusses multilingual datasets Kincaid46, Earnings-21, Earnings-22.",
    "details": [
        {
            "comment": "This file provides details on how data was prepared for evaluation in a research project, including the datasets used and their sources.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/data/README.md\":0-22",
            "content": "This directory supplements the paper with more details on how we prepared the data for evaluation, to help replicate our experiments. \n## Short-form English-only datasets\n### LibriSpeech\nWe used the test-clean and test-other splits from the [LibriSpeech ASR corpus](https://www.openslr.org/12).\n### TED-LIUM 3\nWe used the test split of [TED-LIUM Release 3](https://www.openslr.org/51/), using the segmented manual transcripts included in the release.\n### Common Voice 5.1\nWe downloaded the English subset of Common Voice Corpus 5.1 from [the official website](https://commonvoice.mozilla.org/en/datasets)\n### Artie\nWe used the [Artie bias corpus](https://github.com/artie-inc/artie-bias-corpus). This is a subset of the Common Voice dataset.\n### CallHome & Switchboard\nWe used the two corpora from [LDC2002S09](https://catalog.ldc.upenn.edu/LDC2002S09) and [LDC2002T43](https://catalog.ldc.upenn.edu/LDC2002T43) and followed the [eval2000_data_prep.sh](https://github.com/kaldi-asr/kaldi/blob/master/egs/fisher_"
        },
        {
            "comment": "This code is converting `wav.scp` files to WAV files using bash commands. It is used in the preprocessing of datasets like WSJ, CORAAL, and CHiME-6. The code follows specific recipes for each dataset's preprocessing steps.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/data/README.md\":22-43",
            "content": "swbd/s5/local/eval2000_data_prep.sh) script for preprocessing. The `wav.scp` files can be converted to WAV files with the following bash commands:\n```bash\nmkdir -p wav\nwhile read name cmd; do\n    echo $name\n    echo ${cmd/\\|/} wav/$name.wav | bash\ndone < wav.scp\n```\n### WSJ\nWe used [LDC93S6B](https://catalog.ldc.upenn.edu/LDC93S6B) and [LDC94S13B](https://catalog.ldc.upenn.edu/LDC94S13B) and followed the [s5 recipe](https://github.com/kaldi-asr/kaldi/tree/master/egs/wsj/s5) to preprocess the dataset.\n### CORAAL\nWe used the 231 interviews from [CORAAL (v. 2021.07)](https://oraal.uoregon.edu/coraal) and used the segmentations from [the FairSpeech project](https://github.com/stanford-policylab/asr-disparities/blob/master/input/CORAAL_transcripts.csv).\n### CHiME-6\nWe downloaded the [CHiME-5 dataset](https://spandh.dcs.shef.ac.uk//chime_challenge/CHiME5/download.html) and followed the stage 0 of the [s5_track1 recipe](https://github.com/kaldi-asr/kaldi/tree/master/egs/chime6/s5_track1) to create the CHi"
        },
        {
            "comment": "Code describes data preprocessing steps for various datasets used in the project. It mentions using specific recipes and slicing audio files based on labeled segments.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/data/README.md\":43-60",
            "content": "ME-6 dataset which fixes synchronization. We then used the binaural recordings (`*_P??.wav`) and the corresponding transcripts.\n### AMI-IHM, AMI-SDM1\nWe preprocessed the [AMI Corpus](https://groups.inf.ed.ac.uk/ami/corpus/overview.shtml) by following the stage 0 ad 2 of the [s5b recipe](https://github.com/kaldi-asr/kaldi/tree/master/egs/ami/s5b).\n## Long-form English-only datasets\n### TED-LIUM 3\nTo create a long-form transcription dataset from the [TED-LIUM3](https://www.openslr.org/51/) dataset, we sliced the audio between the beginning of the first labeled segment and the end of the last labeled segment of each talk, and we used the concatenated text as the label. Below are the timestamps used for slicing each of the 11 TED talks in the test split.   \n| Filename            | Begin time (s) | End time (s) |\n|---------------------|----------------|--------------|\n| DanBarber_2010      | 16.09          | 1116.24      |\n| JaneMcGonigal_2010  | 15.476         | 1187.61      |\n| BillGates_2010      | 15.861         | 1656.94      |"
        },
        {
            "comment": "The table contains timings and labels for videos in The Late Show with Stephen Colbert dataset. Meanwhile.json is the source of YouTube video IDs, start and end times, and corrected labels collected from closed-caption data. Rev16 uses a subset of 16 podcast episodes from Rev.AI's Podcast Transcription Benchmark after discovering multiple cases with significant portions affected.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/data/README.md\":61-76",
            "content": "| TomWujec_2010U      | 16.26          | 402.17       |\n| GaryFlake_2010      | 16.06          | 367.14       |\n| EricMead_2009P      | 18.434         | 536.44       |\n| MichaelSpecter_2010 | 16.11          | 979.312      |\n| DanielKahneman_2010 | 15.8           | 1199.44      |\n| AimeeMullins_2009P  | 17.82          | 1296.59      |\n| JamesCameron_2010   | 16.75          | 1010.65      |\n| RobertGupta_2010U   | 16.8           | 387.03       |\n### Meanwhile\nThis dataset consists of 64 segments from The Late Show with Stephen Colbert. The YouTube video ID, start and end timestamps, and the labels can be found in [meanwhile.json](meanwhile.json). The labels are collected from the closed-caption data for each video and corrected with manual inspection.\n### Rev16\nWe use a subset of 16 files from the 30 podcast episodes in [Rev.AI's Podcast Transcription Benchmark](https://www.rev.ai/blog/podcast-transcription-benchmark-part-1/), after finding that there are multiple cases where a significant portion "
        },
        {
            "comment": "The code is listing datasets used in the project. It mentions that 16 episodes were selected from a dataset where audio and labels did not match, and provides a list of file numbers for these episodes. The code then explains that the dataset \"Kincaid46\" consists of 46 audio files and their reference transcripts obtained from an article by Jason Kincaid. For the human transcription benchmark in the paper, 25 examples are used with specific \"Ref IDs\". Two more datasets, Earnings-21 and Earnings-22, were sourced from the speech-datasets repository's 202206 version. Finally, there is a mention of dataset CORAAL, but its description or source is not given in this code chunk.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/data/README.md\":76-92",
            "content": "of the audio and the labels did not match, mostly on the parts introducing the sponsors. We selected 16 episodes that do not have this error, whose \"file number\" are:\n    3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32\n### Kincaid46\nThis dataset consists of 46 audio files and the corresponding transcripts compiled in the blog article [Which automatic transcription service is the most accurate - 2018](https://medium.com/descript/which-automatic-transcription-service-is-the-most-accurate-2018-2e859b23ed19) by Jason Kincaid. We used the 46 audio files and reference transcripts from the Airtable widget in the article.\nFor the human transcription benchmark in the paper, we use a subset of 25 examples from this data, whose \"Ref ID\" are:\n    2 4 5 8 9 10 12 13 14 16 19 21 23 25 26 28 29 30 33 35 36 37 42 43 45\n### Earnings-21, Earnings-22\nFor these datasets, we used the files available in [the speech-datasets repository](https://github.com/revdotcom/speech-datasets), as of their `202206` version.\n### CORAAL"
        },
        {
            "comment": "This code provides information about various multilingual datasets used in the project, such as CORAAL, Multilingual LibriSpeech, Fleurs, VoxPopuli, Common Voice 9, and CoVOST 2. It also mentions how each dataset was collected or obtained by providing links to their respective official repositories or websites.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/data/README.md\":94-117",
            "content": "We used the 231 interviews from [CORAAL (v. 2021.07)](https://oraal.uoregon.edu/coraal) and used the full-length interview files and transcripts.\n## Multilingual datasets\n### Multilingual LibriSpeech\nWe used the test splits from each language in [the Multilingual LibriSpeech (MLS) corpus](https://www.openslr.org/94/).\n### Fleurs\nWe collected audio files and transcripts using the implementation available as [HuggingFace datasets](https://huggingface.co/datasets/google/fleurs/blob/main/fleurs.py). To use as a translation dataset, we matched the numerical utterance IDs to find the corresponding transcript in English.   \n### VoxPopuli\nWe used the `get_asr_data.py` script from [the official repository](https://github.com/facebookresearch/voxpopuli) to collect the ASR data in 14 languages. \n### Common Voice 9\nWe downloaded the Common Voice Corpus 9 from [the official website](https://commonvoice.mozilla.org/en/datasets)\n### CoVOST 2\nWe collected the `X into English` data collected using [the official repository](https://github.com/facebookresearch/covost)."
        }
    ]
}