{
    "summary": "The code applies median filtering and dynamic time warping for comparing sequences, calculates text-to-speech alignment using probabilities, removes hooks, normalizes weights, and merges words with punctuation in an alignment. It also adjusts word timings, considers sentence end marks, and stores probabilities.",
    "details": [
        {
            "comment": "This function applies a median filter to the input tensor `x` along the last dimension, specified by `filter_width`. It also handles padding and supports 3D or 4D inputs.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":0-35",
            "content": "import itertools\nimport subprocess\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, List\nimport numba\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom .audio import HOP_LENGTH, SAMPLE_RATE, TOKENS_PER_SECOND\nfrom .tokenizer import Tokenizer\nif TYPE_CHECKING:\n    from .model import Whisper\ndef median_filter(x: torch.Tensor, filter_width: int):\n    \"\"\"Apply a median filter of width `filter_width` along the last dimension of `x`\"\"\"\n    pad_width = filter_width // 2\n    if x.shape[-1] <= pad_width:\n        # F.pad requires the padding width to be smaller than the input dimension\n        return x\n    if (ndim := x.ndim) <= 2:\n        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D\n        x = x[None, None, :]\n    assert (\n        filter_width > 0 and filter_width % 2 == 1\n    ), \"`filter_width` should be an odd number\"\n    result = None\n    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=\"reflect\")\n    if x.is_cuda:"
        },
        {
            "comment": "The code attempts to apply a median filter with CUDA if available, and falls back to a slower implementation otherwise. It also includes a function for backtracking through the filtered data.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":36-72",
            "content": "        try:\n            from .triton_ops import median_filter_cuda\n            result = median_filter_cuda(x, filter_width)\n        except (RuntimeError, subprocess.CalledProcessError):\n            warnings.warn(\n                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n                \"falling back to a slower median kernel implementation...\"\n            )\n    if result is None:\n        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)\n        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]\n    if ndim <= 2:\n        result = result[0, 0]\n    return result\n@numba.jit(nopython=True)\ndef backtrace(trace: np.ndarray):\n    i = trace.shape[0] - 1\n    j = trace.shape[1] - 1\n    trace[0, :] = 2\n    trace[:, 0] = 1\n    result = []\n    while i > 0 or j > 0:\n        result.append((i - 1, j - 1))\n        if trace[i, j] == 0:\n            i -= 1\n            j -= 1\n        elif trace[i, j] == 1:\n            i -= 1\n        elif trace[i, j] == 2:"
        },
        {
            "comment": "This code contains two functions, `dtw_cpu` and `dtw_cuda`, which implement dynamic time warping (DTW) algorithms using different backends. The `dtw_cpu` function implements the DTW algorithm on the CPU using numpy, while the `dtw_cuda` function uses CUDA for GPU acceleration. The code defines variables, initializes arrays, and performs calculations based on the chosen backend.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":73-114",
            "content": "            j -= 1\n        else:\n            raise ValueError(\"Unexpected trace[i, j]\")\n    result = np.array(result)\n    return result[::-1, :].T\n@numba.jit(nopython=True, parallel=True)\ndef dtw_cpu(x: np.ndarray):\n    N, M = x.shape\n    cost = np.ones((N + 1, M + 1), dtype=np.float32) * np.inf\n    trace = -np.ones((N + 1, M + 1), dtype=np.float32)\n    cost[0, 0] = 0\n    for j in range(1, M + 1):\n        for i in range(1, N + 1):\n            c0 = cost[i - 1, j - 1]\n            c1 = cost[i - 1, j]\n            c2 = cost[i, j - 1]\n            if c0 < c1 and c0 < c2:\n                c, t = c0, 0\n            elif c1 < c0 and c1 < c2:\n                c, t = c1, 1\n            else:\n                c, t = c2, 2\n            cost[i, j] = x[i - 1, j - 1] + c\n            trace[i, j] = t\n    return backtrace(trace)\ndef dtw_cuda(x, BLOCK_SIZE=1024):\n    from .triton_ops import dtw_kernel\n    M, N = x.shape\n    assert M < BLOCK_SIZE, f\"M should be smaller than {BLOCK_SIZE=}\"\n    x_skew = (\n        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)"
        },
        {
            "comment": "This code implements the dynamic time warping (DTW) algorithm for comparing sequences of data points. The code includes CUDA-accelerated and CPU versions of DTW functions, as well as a class for storing word timing information.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":115-159",
            "content": "    )\n    x_skew = x_skew.T.contiguous()\n    cost = torch.ones(N + M + 2, M + 2) * np.inf\n    cost[0, 0] = 0\n    cost = cost.cuda()\n    trace = torch.zeros_like(cost, dtype=torch.int32)\n    dtw_kernel[(1,)](\n        cost,\n        trace,\n        x_skew,\n        x_skew.stride(0),\n        cost.stride(0),\n        trace.stride(0),\n        N,\n        M,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[\n        :, : N + 1\n    ]\n    return backtrace(trace.cpu().numpy())\ndef dtw(x: torch.Tensor) -> np.ndarray:\n    if x.is_cuda:\n        try:\n            return dtw_cuda(x)\n        except (RuntimeError, subprocess.CalledProcessError):\n            warnings.warn(\n                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n                \"falling back to a slower DTW implementation...\"\n            )\n    return dtw_cpu(x.double().cpu().numpy())\n@dataclass\nclass WordTiming:\n    word: str\n    tokens: List[int]\n    start: float\n    end: float\n    probability: float"
        },
        {
            "comment": "This function finds the alignment between model output and input text tokens. It first creates a tensor of tokens including start-of-text (SOT), no timestamp, text tokens, and end-of-text (EOT). Then, it installs hooks on cross attention layers to retrieve attention weights. Finally, it computes logits and token probabilities using the model and tokenizer.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":162-196",
            "content": "def find_alignment(\n    model: \"Whisper\",\n    tokenizer: Tokenizer,\n    text_tokens: List[int],\n    mel: torch.Tensor,\n    num_frames: int,\n    *,\n    medfilt_width: int = 7,\n    qk_scale: float = 1.0,\n) -> List[WordTiming]:\n    if len(text_tokens) == 0:\n        return []\n    tokens = torch.tensor(\n        [\n            *tokenizer.sot_sequence,\n            tokenizer.no_timestamps,\n            *text_tokens,\n            tokenizer.eot,\n        ]\n    ).to(model.device)\n    # install hooks on the cross attention layers to retrieve the attention weights\n    QKs = [None] * model.dims.n_text_layer\n    hooks = [\n        block.cross_attn.register_forward_hook(\n            lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1][0])\n        )\n        for i, block in enumerate(model.decoder.blocks)\n    ]\n    with torch.no_grad():\n        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))[0]\n        sampled_logits = logits[len(tokenizer.sot_sequence) :, : tokenizer.eot]\n        token_probs = sampled_logits.softmax(dim=-1)"
        },
        {
            "comment": "This code calculates text-to-speech alignment using Dynamic Time Warping (DTW) algorithm. It first computes text and time token probabilities, removes hooks if any, stacks the query and key matrices from the model's alignment heads, selects relevant frames, normalizes the weights, applies median filtering, calculates mean matrix, excludes start-of-text (SOT) and end-of-text (EOT) tokens, computes DTW using text and time indices, splits text into word tokens, and finally returns if there is only one word token.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":197-221",
            "content": "        text_token_probs = token_probs[np.arange(len(text_tokens)), text_tokens]\n        text_token_probs = text_token_probs.tolist()\n    for hook in hooks:\n        hook.remove()\n    # heads * tokens * frames\n    weights = torch.stack([QKs[_l][_h] for _l, _h in model.alignment_heads.indices().T])\n    weights = weights[:, :, : num_frames // 2]\n    weights = (weights * qk_scale).softmax(dim=-1)\n    std, mean = torch.std_mean(weights, dim=-2, keepdim=True, unbiased=False)\n    weights = (weights - mean) / std\n    weights = median_filter(weights, medfilt_width)\n    matrix = weights.mean(axis=0)\n    matrix = matrix[len(tokenizer.sot_sequence) : -1]\n    text_indices, time_indices = dtw(-matrix)\n    words, word_tokens = tokenizer.split_to_word_tokens(text_tokens + [tokenizer.eot])\n    if len(word_tokens) <= 1:\n        # return on eot only\n        # >>> np.pad([], (1, 0))\n        # array([0.])\n        # This results in crashes when we lookup jump_times with float, like\n        # IndexError: arrays used as indices must be of integer (or boolean) type"
        },
        {
            "comment": "Function to calculate timing and probabilities for each word in the text\n\nThis function calculates start and end times for each word, as well as their probability. It returns a list of WordTiming objects containing word information with their respective timings and probabilities.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":222-249",
            "content": "        return []\n    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n    jumps = np.pad(np.diff(text_indices), (1, 0), constant_values=1).astype(bool)\n    jump_times = time_indices[jumps] / TOKENS_PER_SECOND\n    start_times = jump_times[word_boundaries[:-1]]\n    end_times = jump_times[word_boundaries[1:]]\n    word_probabilities = [\n        np.mean(text_token_probs[i:j])\n        for i, j in zip(word_boundaries[:-1], word_boundaries[1:])\n    ]\n    return [\n        WordTiming(word, tokens, start, end, probability)\n        for word, tokens, start, end, probability in zip(\n            words, word_tokens, start_times, end_times, word_probabilities\n        )\n    ]\ndef merge_punctuations(alignment: List[WordTiming], prepended: str, appended: str):\n    # merge prepended punctuations\n    i = len(alignment) - 2\n    j = len(alignment) - 1\n    while i >= 0:\n        previous = alignment[i]\n        following = alignment[j]\n        if previous.word.startswith(\" \") and previous.word.strip() in prepended:"
        },
        {
            "comment": "This code is responsible for merging words and timestamps in a given alignment. It first prepends the next word with any previously appended punctuations, then iterates over the alignment to merge words that follow non-space ending words with appended punctuations by concatenating them. This function takes segments, model, tokenizer, mel spectrogram, number of frames, and optional parameters for prepend and append punctuation.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":250-284",
            "content": "            # prepend it to the following word\n            following.word = previous.word + following.word\n            following.tokens = previous.tokens + following.tokens\n            previous.word = \"\"\n            previous.tokens = []\n        else:\n            j = i\n        i -= 1\n    # merge appended punctuations\n    i = 0\n    j = 1\n    while j < len(alignment):\n        previous = alignment[i]\n        following = alignment[j]\n        if not previous.word.endswith(\" \") and following.word in appended:\n            # append it to the previous word\n            previous.word = previous.word + following.word\n            previous.tokens = previous.tokens + following.tokens\n            following.word = \"\"\n            following.tokens = []\n        else:\n            i = j\n        j += 1\ndef add_word_timestamps(\n    *,\n    segments: List[dict],\n    model: \"Whisper\",\n    tokenizer: Tokenizer,\n    mel: torch.Tensor,\n    num_frames: int,\n    prepend_punctuations: str = \"\\\"'\u201c\u00bf([{-\",\n    append_punctuations: str = \"\\\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\","
        },
        {
            "comment": "Calculates median word duration and truncates long words at sentence boundaries for better speech synthesis.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":285-309",
            "content": "    last_speech_timestamp: float,\n    **kwargs,\n):\n    if len(segments) == 0:\n        return\n    text_tokens_per_segment = [\n        [token for token in segment[\"tokens\"] if token < tokenizer.eot]\n        for segment in segments\n    ]\n    text_tokens = list(itertools.chain.from_iterable(text_tokens_per_segment))\n    alignment = find_alignment(model, tokenizer, text_tokens, mel, num_frames, **kwargs)\n    word_durations = np.array([t.end - t.start for t in alignment])\n    word_durations = word_durations[word_durations.nonzero()]\n    median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0\n    median_duration = min(0.7, float(median_duration))\n    max_duration = median_duration * 2\n    # hack: truncate long words at sentence boundaries.\n    # a better segmentation algorithm based on VAD should be able to replace this.\n    if len(word_durations) > 0:\n        sentence_end_marks = \".\u3002!\uff01?\uff1f\"\n        # ensure words at sentence boundaries are not longer than twice the median word duration.\n        for i in range(1, len(alignment)):"
        },
        {
            "comment": "This code is adjusting the timing of words in an alignment list based on a maximum duration and sentence end marks. It also calculates time offsets for segments and extracts words from the alignment, storing their start and end times along with probabilities.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":310-334",
            "content": "            if alignment[i].end - alignment[i].start > max_duration:\n                if alignment[i].word in sentence_end_marks:\n                    alignment[i].end = alignment[i].start + max_duration\n                elif alignment[i - 1].word in sentence_end_marks:\n                    alignment[i].start = alignment[i].end - max_duration\n    merge_punctuations(alignment, prepend_punctuations, append_punctuations)\n    time_offset = segments[0][\"seek\"] * HOP_LENGTH / SAMPLE_RATE\n    word_index = 0\n    for segment, text_tokens in zip(segments, text_tokens_per_segment):\n        saved_tokens = 0\n        words = []\n        while word_index < len(alignment) and saved_tokens < len(text_tokens):\n            timing = alignment[word_index]\n            if timing.word:\n                words.append(\n                    dict(\n                        word=timing.word,\n                        start=round(time_offset + timing.start, 2),\n                        end=round(time_offset + timing.end, 2),\n                        probability=timing.probability,"
        },
        {
            "comment": "This code is performing a segmentation algorithm for speech by truncating long words at segment boundaries. It checks the duration of words and ensures that no word exceeds twice the median word duration or that the second word after a pause does not exceed the maximum duration. If necessary, it adjusts the word endings to fit these criteria.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":335-358",
            "content": "                    )\n                )\n            saved_tokens += len(timing.tokens)\n            word_index += 1\n        # hack: truncate long words at segment boundaries.\n        # a better segmentation algorithm based on VAD should be able to replace this.\n        if len(words) > 0:\n            # ensure the first and second word after a pause is not longer than\n            # twice the median word duration.\n            if words[0][\"end\"] - last_speech_timestamp > median_duration * 4 and (\n                words[0][\"end\"] - words[0][\"start\"] > max_duration\n                or (\n                    len(words) > 1\n                    and words[1][\"end\"] - words[0][\"start\"] > max_duration * 2\n                )\n            ):\n                if (\n                    len(words) > 1\n                    and words[1][\"end\"] - words[1][\"start\"] > max_duration\n                ):\n                    boundary = max(words[1][\"end\"] / 2, words[1][\"end\"] - max_duration)\n                    words[0][\"end\"] = words[1][\"start\"] = boundary"
        },
        {
            "comment": "This code adjusts word start and end timestamps to fit within the segment timestamps. It prefers segment-level timestamps if a word extends beyond the segment, ensuring the segment remains complete.",
            "location": "\"/media/root/Toshiba XG3/works/whisper/docs/src/whisper/timing.py\":359-385",
            "content": "                words[0][\"start\"] = max(0, words[0][\"end\"] - max_duration)\n            # prefer the segment-level start timestamp if the first word is too long.\n            if (\n                segment[\"start\"] < words[0][\"end\"]\n                and segment[\"start\"] - 0.5 > words[0][\"start\"]\n            ):\n                words[0][\"start\"] = max(\n                    0, min(words[0][\"end\"] - median_duration, segment[\"start\"])\n                )\n            else:\n                segment[\"start\"] = words[0][\"start\"]\n            # prefer the segment-level end timestamp if the last word is too long.\n            if (\n                segment[\"end\"] > words[-1][\"start\"]\n                and segment[\"end\"] + 0.5 < words[-1][\"end\"]\n            ):\n                words[-1][\"end\"] = max(\n                    words[-1][\"start\"] + median_duration, segment[\"end\"]\n                )\n            else:\n                segment[\"end\"] = words[-1][\"end\"]\n            last_speech_timestamp = segment[\"end\"]\n        segment[\"words\"] = words"
        }
    ]
}