{
    "300": {
        "file_id": 21,
        "content": "    \"as\": \"assamese\",\n    \"tt\": \"tatar\",\n    \"haw\": \"hawaiian\",\n    \"ln\": \"lingala\",\n    \"ha\": \"hausa\",\n    \"ba\": \"bashkir\",\n    \"jw\": \"javanese\",\n    \"su\": \"sundanese\",\n    \"yue\": \"cantonese\",\n}\n# language code lookup by name, with a few language aliases\nTO_LANGUAGE_CODE = {\n    **{language: code for code, language in LANGUAGES.items()},\n    \"burmese\": \"my\",\n    \"valencian\": \"ca\",\n    \"flemish\": \"nl\",\n    \"haitian\": \"ht\",\n    \"letzeburgesch\": \"lb\",\n    \"pushto\": \"ps\",\n    \"panjabi\": \"pa\",\n    \"moldavian\": \"ro\",\n    \"moldovan\": \"ro\",\n    \"sinhalese\": \"si\",\n    \"castilian\": \"es\",\n    \"mandarin\": \"zh\",\n}\n@dataclass\nclass Tokenizer:\n    \"\"\"A thin wrapper around `tiktoken` providing quick access to special tokens\"\"\"\n    encoding: tiktoken.Encoding\n    num_languages: int\n    language: Optional[str] = None\n    task: Optional[str] = None\n    sot_sequence: Tuple[int] = ()\n    special_tokens: Dict[str, int] = field(default_factory=dict)\n    def __post_init__(self):\n        for special in self.encoding.special_tokens_set:\n            special_token = self.encoding.encode_single_token(special)",
        "type": "code",
        "location": "/whisper/tokenizer.py:102-144"
    },
    "301": {
        "file_id": 21,
        "content": "This code defines a class called Tokenizer that wraps around the `tiktoken` library. It provides quick access to special tokens and supports specifying a language, task, and start-of-text (SOT) sequence. The code also includes a dictionary mapping language aliases to their respective language codes for easier lookup.",
        "type": "comment"
    },
    "302": {
        "file_id": 21,
        "content": "            self.special_tokens[special] = special_token\n        sot: int = self.special_tokens[\"<|startoftranscript|>\"]\n        translate: int = self.special_tokens[\"<|translate|>\"]\n        transcribe: int = self.special_tokens[\"<|transcribe|>\"]\n        langs = tuple(LANGUAGES.keys())[: self.num_languages]\n        sot_sequence = [sot]\n        if self.language is not None:\n            sot_sequence.append(sot + 1 + langs.index(self.language))\n        if self.task is not None:\n            task_token: int = transcribe if self.task == \"transcribe\" else translate\n            sot_sequence.append(task_token)\n        self.sot_sequence = tuple(sot_sequence)\n    def encode(self, text, **kwargs):\n        return self.encoding.encode(text, **kwargs)\n    def decode(self, token_ids: List[int], **kwargs) -> str:\n        token_ids = [t for t in token_ids if t < self.timestamp_begin]\n        return self.encoding.decode(token_ids, **kwargs)\n    def decode_with_timestamps(self, token_ids: List[int], **kwargs) -> str:\n        \"\"\"",
        "type": "code",
        "location": "/whisper/tokenizer.py:145-169"
    },
    "303": {
        "file_id": 21,
        "content": "This code defines a class for tokenizing text with special tokens, language information, and task information. It also includes methods to encode and decode tokenized text.",
        "type": "comment"
    },
    "304": {
        "file_id": 21,
        "content": "        Timestamp tokens are above other special tokens' id range and are ignored by `decode()`.\n        This method decodes given tokens with timestamps tokens annotated, e.g. \"<|1.08|>\".\n        \"\"\"\n        return self.encoding.decode(token_ids, **kwargs)\n    @cached_property\n    def eot(self) -> int:\n        return self.encoding.eot_token\n    @cached_property\n    def transcribe(self) -> int:\n        return self.special_tokens[\"<|transcribe|>\"]\n    @cached_property\n    def translate(self) -> int:\n        return self.special_tokens[\"<|translate|>\"]\n    @cached_property\n    def sot(self) -> int:\n        return self.special_tokens[\"<|startoftranscript|>\"]\n    @cached_property\n    def sot_lm(self) -> int:\n        return self.special_tokens[\"<|startoflm|>\"]\n    @cached_property\n    def sot_prev(self) -> int:\n        return self.special_tokens[\"<|startofprev|>\"]\n    @cached_property\n    def no_speech(self) -> int:\n        return self.special_tokens[\"<|nospeech|>\"]\n    @cached_property\n    def no_timestamps(self) -> int:",
        "type": "code",
        "location": "/whisper/tokenizer.py:170-204"
    },
    "305": {
        "file_id": 21,
        "content": "The code defines a tokenizer class with properties for various special tokens like end-of-transcript (eot), transcribe, translate, start of transcript (sot), start of language model (sot_lm), start of previous transcript (sot_prev), no speech, and no timestamps. The decode method decodes given tokens with timestamps annotated using the encoding's decode function.",
        "type": "comment"
    },
    "306": {
        "file_id": 21,
        "content": "        return self.special_tokens[\"<|notimestamps|>\"]\n    @cached_property\n    def timestamp_begin(self) -> int:\n        return self.special_tokens[\"<|0.00|>\"]\n    @cached_property\n    def language_token(self) -> int:\n        \"\"\"Returns the token id corresponding to the value of the `language` field\"\"\"\n        if self.language is None:\n            raise ValueError(\"This tokenizer does not have language token configured\")\n        return self.to_language_token(self.language)\n    def to_language_token(self, language):\n        if token := self.special_tokens.get(f\"<|{language}|>\", None):\n            return token\n        raise KeyError(f\"Language {language} not found in tokenizer.\")\n    @cached_property\n    def all_language_tokens(self) -> Tuple[int]:\n        result = []\n        for token, token_id in self.special_tokens.items():\n            if token.strip(\"<|>\") in LANGUAGES:\n                result.append(token_id)\n        return tuple(result)[: self.num_languages]\n    @cached_property\n    def all_language_codes(self) -> Tuple[str]:",
        "type": "code",
        "location": "/whisper/tokenizer.py:205-234"
    },
    "307": {
        "file_id": 21,
        "content": "This code is a part of a tokenizer class for a language model. It includes properties and methods to handle special tokens such as \"<|notimestamps|>\", timestamp tokens like \"<|0.00|>\", and language-specific tokens. The `to_language_token` method returns the token id corresponding to the input language, while `all_language_tokens` and `all_language_codes` return tuples of all language tokens and codes respectively.",
        "type": "comment"
    },
    "308": {
        "file_id": 21,
        "content": "        return tuple(self.decode([_l]).strip(\"<|>\") for _l in self.all_language_tokens)\n    @cached_property\n    def sot_sequence_including_notimestamps(self) -> Tuple[int]:\n        return tuple(list(self.sot_sequence) + [self.no_timestamps])\n    @cached_property\n    def non_speech_tokens(self) -> Tuple[int]:\n        \"\"\"\n        Returns the list of tokens to suppress in order to avoid any speaker tags or non-speech\n        annotations, to prevent sampling texts that are not actually spoken in the audio, e.g.\n        - ♪♪♪\n        - ( SPEAKING FOREIGN LANGUAGE )\n        - [DAVID] Hey there,\n        keeping basic punctuations like commas, periods, question marks, exclamation points, etc.\n        \"\"\"\n        symbols = list('\"#()*+/:;<=>@[\\\\]^_`{|}~「」『』')\n        symbols += (\n            \"<< >> <<< >>> -- --- -( -[ (' (\\\" (( )) ((( ))) [[ ]] {{ }} ♪♪ ♪♪♪\".split()\n        )\n        # symbols that may be a single token or multiple tokens depending on the tokenizer.\n        # In case they're multiple tokens, suppress the first token, which is safe because:",
        "type": "code",
        "location": "/whisper/tokenizer.py:235-259"
    },
    "309": {
        "file_id": 21,
        "content": "This code defines a tokenizer class with various properties and methods for handling language tokens, speaker tags, non-speech annotations, and punctuations. It includes functions to decode language tokens, create a sequence of start of text (SOT) including no time stamps, and a list of non-speech tokens to suppress for accurate sampling of spoken audio.",
        "type": "comment"
    },
    "310": {
        "file_id": 21,
        "content": "        # These are between U+2640 and U+267F miscellaneous symbols that are okay to suppress\n        # in generations, and in the 3-byte UTF-8 representation they share the first two bytes.\n        miscellaneous = set(\"♩♪♫♬♭♮♯\")\n        assert all(0x2640 <= ord(c) <= 0x267F for c in miscellaneous)\n        # allow hyphens \"-\" and single quotes \"'\" between words, but not at the beginning of a word\n        result = {self.encoding.encode(\" -\")[0], self.encoding.encode(\" '\")[0]}\n        for symbol in symbols + list(miscellaneous):\n            for tokens in [\n                self.encoding.encode(symbol),\n                self.encoding.encode(\" \" + symbol),\n            ]:\n                if len(tokens) == 1 or symbol in miscellaneous:\n                    result.add(tokens[0])\n        return tuple(sorted(result))\n    def split_to_word_tokens(self, tokens: List[int]):\n        if self.language in {\"zh\", \"ja\", \"th\", \"lo\", \"my\", \"yue\"}:\n            # These languages don't typically use spaces, so it is difficult to split words",
        "type": "code",
        "location": "/whisper/tokenizer.py:260-279"
    },
    "311": {
        "file_id": 21,
        "content": "This function creates a set of characters to suppress and allows hyphens and single quotes between words. It then encodes various symbols, checks if they are 1 character long or in the miscellaneous set, and adds them to the result set. Finally, it sorts and returns the result set.",
        "type": "comment"
    },
    "312": {
        "file_id": 21,
        "content": "            # without morpheme analysis. Here, we instead split words at any\n            # position where the tokens are decoded as valid unicode points\n            return self.split_tokens_on_unicode(tokens)\n        return self.split_tokens_on_spaces(tokens)\n    def split_tokens_on_unicode(self, tokens: List[int]):\n        decoded_full = self.decode_with_timestamps(tokens)\n        replacement_char = \"\\ufffd\"\n        words = []\n        word_tokens = []\n        current_tokens = []\n        unicode_offset = 0\n        for token in tokens:\n            current_tokens.append(token)\n            decoded = self.decode_with_timestamps(current_tokens)\n            if (\n                replacement_char not in decoded\n                or decoded_full[unicode_offset + decoded.index(replacement_char)]\n                == replacement_char\n            ):\n                words.append(decoded)\n                word_tokens.append(current_tokens)\n                current_tokens = []\n                unicode_offset += len(decoded)\n        return words, word_tokens",
        "type": "code",
        "location": "/whisper/tokenizer.py:280-309"
    },
    "313": {
        "file_id": 21,
        "content": "This code splits tokens into words by checking if they can be decoded as valid unicode points. If a token can't be decoded, it considers the preceding word complete and moves on to the next word. It returns both the decoded text (words) and the original token list (word_tokens).",
        "type": "comment"
    },
    "314": {
        "file_id": 21,
        "content": "    def split_tokens_on_spaces(self, tokens: List[int]):\n        subwords, subword_tokens_list = self.split_tokens_on_unicode(tokens)\n        words = []\n        word_tokens = []\n        for subword, subword_tokens in zip(subwords, subword_tokens_list):\n            special = subword_tokens[0] >= self.eot\n            with_space = subword.startswith(\" \")\n            punctuation = subword.strip() in string.punctuation\n            if special or with_space or punctuation or len(words) == 0:\n                words.append(subword)\n                word_tokens.append(subword_tokens)\n            else:\n                words[-1] = words[-1] + subword\n                word_tokens[-1].extend(subword_tokens)\n        return words, word_tokens\n@lru_cache(maxsize=None)\ndef get_encoding(name: str = \"gpt2\", num_languages: int = 99):\n    vocab_path = os.path.join(os.path.dirname(__file__), \"assets\", f\"{name}.tiktoken\")\n    ranks = {\n        base64.b64decode(token): int(rank)\n        for token, rank in (line.split() for line in open(vocab_path) if line)",
        "type": "code",
        "location": "/whisper/tokenizer.py:311-335"
    },
    "315": {
        "file_id": 21,
        "content": "This code splits tokens into subwords, and then further splits them based on special characters, words with spaces, or punctuation. It returns a list of words and corresponding token lists. The `get_encoding` function loads vocabulary information from a file and returns a function for encoding strings to tokens based on the specified name and number of languages.",
        "type": "comment"
    },
    "316": {
        "file_id": 21,
        "content": "    }\n    n_vocab = len(ranks)\n    special_tokens = {}\n    specials = [\n        \"<|endoftext|>\",\n        \"<|startoftranscript|>\",\n        *[f\"<|{lang}|>\" for lang in list(LANGUAGES.keys())[:num_languages]],\n        \"<|translate|>\",\n        \"<|transcribe|>\",\n        \"<|startoflm|>\",\n        \"<|startofprev|>\",\n        \"<|nospeech|>\",\n        \"<|notimestamps|>\",\n        *[f\"<|{i * 0.02:.2f}|>\" for i in range(1501)],\n    ]\n    for token in specials:\n        special_tokens[token] = n_vocab\n        n_vocab += 1\n    return tiktoken.Encoding(\n        name=os.path.basename(vocab_path),\n        explicit_n_vocab=n_vocab,\n        pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n        mergeable_ranks=ranks,\n        special_tokens=special_tokens,\n    )\n@lru_cache(maxsize=None)\ndef get_tokenizer(\n    multilingual: bool,\n    *,\n    num_languages: int = 99,\n    language: Optional[str] = None,\n    task: Optional[str] = None,  # Literal[\"transcribe\", \"translate\", None]\n) -> Tokenizer:\n    if language is not None:",
        "type": "code",
        "location": "/whisper/tokenizer.py:336-374"
    },
    "317": {
        "file_id": 21,
        "content": "This code defines a function that returns a tokenizer object based on specified parameters such as multilingual, num_languages, and language. The returned tokenizer is implemented using the tiktoken Encoding class and includes special tokens for tasks like translation and transcribe. The code also handles end-of-text and start-of-transcript markers, different languages, and specific time intervals. It creates a list of special tokens and assigns their corresponding ranks in the mergeable_ranks parameter. Additionally, it caches the function to improve performance by using the @lru_cache decorator.",
        "type": "comment"
    },
    "318": {
        "file_id": 21,
        "content": "        language = language.lower()\n        if language not in LANGUAGES:\n            if language in TO_LANGUAGE_CODE:\n                language = TO_LANGUAGE_CODE[language]\n            else:\n                raise ValueError(f\"Unsupported language: {language}\")\n    if multilingual:\n        encoding_name = \"multilingual\"\n        language = language or \"en\"\n        task = task or \"transcribe\"\n    else:\n        encoding_name = \"gpt2\"\n        language = None\n        task = None\n    encoding = get_encoding(name=encoding_name, num_languages=num_languages)\n    return Tokenizer(\n        encoding=encoding, num_languages=num_languages, language=language, task=task\n    )",
        "type": "code",
        "location": "/whisper/tokenizer.py:375-395"
    },
    "319": {
        "file_id": 21,
        "content": "This code checks the given language and initializes a tokenizer based on its properties. If the language is not supported, it raises an error. The encoding name and task are determined based on whether the input is multilingual or not. Then, it returns a tokenizer object with the specified parameters.",
        "type": "comment"
    },
    "320": {
        "file_id": 22,
        "content": "/whisper/transcribe.py",
        "type": "filepath"
    },
    "321": {
        "file_id": 22,
        "content": "The code handles speech transcription with Whisper model libraries, providing customizable parameters and managing segments, silence, and seek values. It transcribes audio files with timestamps, punctuation, and function options while checking input language and detecting hallucination using silence thresholds.",
        "type": "summary"
    },
    "322": {
        "file_id": 22,
        "content": "import argparse\nimport os\nimport traceback\nimport warnings\nfrom typing import TYPE_CHECKING, List, Optional, Tuple, Union\nimport numpy as np\nimport torch\nimport tqdm\nfrom .audio import (\n    FRAMES_PER_SECOND,\n    HOP_LENGTH,\n    N_FRAMES,\n    N_SAMPLES,\n    SAMPLE_RATE,\n    log_mel_spectrogram,\n    pad_or_trim,\n)\nfrom .decoding import DecodingOptions, DecodingResult\nfrom .timing import add_word_timestamps\nfrom .tokenizer import LANGUAGES, TO_LANGUAGE_CODE, get_tokenizer\nfrom .utils import (\n    exact_div,\n    format_timestamp,\n    get_end,\n    get_writer,\n    make_safe,\n    optional_float,\n    optional_int,\n    str2bool,\n)\nif TYPE_CHECKING:\n    from .model import Whisper\ndef transcribe(\n    model: \"Whisper\",\n    audio: Union[str, np.ndarray, torch.Tensor],\n    *,\n    verbose: Optional[bool] = None,\n    temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    compression_ratio_threshold: Optional[float] = 2.4,\n    logprob_threshold: Optional[float] = -1.0,\n    no_speech_threshold: Optional[float] = 0.6,",
        "type": "code",
        "location": "/whisper/transcribe.py:1-46"
    },
    "323": {
        "file_id": 22,
        "content": "This code is importing necessary libraries and modules for the Whisper model to transcribe audio. It defines a function `transcribe` that takes in an audio file, optional arguments for verbose, temperature, compression_ratio_threshold, logprob_threshold, and no_speech_threshold. The function uses the Whisper model to transcribe the given audio.",
        "type": "comment"
    },
    "324": {
        "file_id": 22,
        "content": "    condition_on_previous_text: bool = True,\n    initial_prompt: Optional[str] = None,\n    word_timestamps: bool = False,\n    prepend_punctuations: str = \"\\\"'“¿([{-\",\n    append_punctuations: str = \"\\\"'.。,，!！?？:：”)]}、\",\n    clip_timestamps: Union[str, List[float]] = \"0\",\n    hallucination_silence_threshold: Optional[float] = None,\n    **decode_options,\n):\n    \"\"\"\n    Transcribe an audio file using Whisper\n    Parameters\n    ----------\n    model: Whisper\n        The Whisper model instance\n    audio: Union[str, np.ndarray, torch.Tensor]\n        The path to the audio file to open, or the audio waveform\n    verbose: bool\n        Whether to display the text being decoded to the console. If True, displays all the details,\n        If False, displays minimal details. If None, does not display anything\n    temperature: Union[float, Tuple[float, ...]]\n        Temperature for sampling. It can be a tuple of temperatures, which will be successively used\n        upon failures according to either `compression_ratio_threshold` or `logprob_threshold`.",
        "type": "code",
        "location": "/whisper/transcribe.py:47-73"
    },
    "325": {
        "file_id": 22,
        "content": "This function transcribes an audio file using Whisper. It takes parameters like the model instance, audio file path or waveform, verbosity level for displaying text being decoded, and temperatures for sampling during the transcription process.",
        "type": "comment"
    },
    "326": {
        "file_id": 22,
        "content": "    compression_ratio_threshold: float\n        If the gzip compression ratio is above this value, treat as failed\n    logprob_threshold: float\n        If the average log probability over sampled tokens is below this value, treat as failed\n    no_speech_threshold: float\n        If the no_speech probability is higher than this value AND the average log probability\n        over sampled tokens is below `logprob_threshold`, consider the segment as silent\n    condition_on_previous_text: bool\n        if True, the previous output of the model is provided as a prompt for the next window;\n        disabling may make the text inconsistent across windows, but the model becomes less prone to\n        getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n    word_timestamps: bool\n        Extract word-level timestamps using the cross-attention pattern and dynamic time warping,\n        and include the timestamps for each word in each segment.\n    prepend_punctuations: str\n        If word_timestamps is True, merge these punctuation symbols with the next word",
        "type": "code",
        "location": "/whisper/transcribe.py:75-95"
    },
    "327": {
        "file_id": 22,
        "content": "These variables define the thresholds and conditions for determining if a segment is successful or not, whether to condition on previous text, extract word-level timestamps, and how to handle punctuation symbols in the output.",
        "type": "comment"
    },
    "328": {
        "file_id": 22,
        "content": "    append_punctuations: str\n        If word_timestamps is True, merge these punctuation symbols with the previous word\n    initial_prompt: Optional[str]\n        Optional text to provide as a prompt for the first window. This can be used to provide, or\n        \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n        to make it more likely to predict those word correctly.\n    decode_options: dict\n        Keyword arguments to construct `DecodingOptions` instances\n    clip_timestamps: Union[str, List[float]]\n        Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process.\n        The last end timestamp defaults to the end of the file.\n    hallucination_silence_threshold: Optional[float]\n        When word_timestamps is True, skip silent periods longer than this threshold (in seconds)\n        when a possible hallucination is detected\n    Returns\n    -------\n    A dictionary containing the resulting text (\"text\") and segment-level details (\"segments\"), and",
        "type": "code",
        "location": "/whisper/transcribe.py:97-118"
    },
    "329": {
        "file_id": 22,
        "content": "This function takes optional parameters for prompt, decoding options, clip timestamps, and hallucination silence threshold to transcribe speech into text. It returns a dictionary containing the resulting text and segment-level details.",
        "type": "comment"
    },
    "330": {
        "file_id": 22,
        "content": "    the spoken language (\"language\"), which is detected when `decode_options[\"language\"]` is None.\n    \"\"\"\n    dtype = torch.float16 if decode_options.get(\"fp16\", True) else torch.float32\n    if model.device == torch.device(\"cpu\"):\n        if torch.cuda.is_available():\n            warnings.warn(\"Performing inference on CPU when CUDA is available\")\n        if dtype == torch.float16:\n            warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n            dtype = torch.float32\n    if dtype == torch.float32:\n        decode_options[\"fp16\"] = False\n    # Pad 30-seconds of silence to the input audio, for slicing\n    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n    content_frames = mel.shape[-1] - N_FRAMES\n    content_duration = float(content_frames * HOP_LENGTH / SAMPLE_RATE)\n    if decode_options.get(\"language\", None) is None:\n        if not model.is_multilingual:\n            decode_options[\"language\"] = \"en\"\n        else:\n            if verbose:\n                print(",
        "type": "code",
        "location": "/whisper/transcribe.py:119-142"
    },
    "331": {
        "file_id": 22,
        "content": "This code snippet is preparing the audio input for inference by handling the data type, padding silence to the input audio, and setting the language for decoding.\nThe code checks if the device is CPU or GPU and adjusts the data type accordingly. It also adds 30 seconds of silence to the input audio for slicing and calculates the content duration based on the frame size, hop length, and sample rate. If no language is specified in the decode options, it sets a default language if the model is not multilingual or displays a message if the model is multilingual.",
        "type": "comment"
    },
    "332": {
        "file_id": 22,
        "content": "                    \"Detecting language using up to the first 30 seconds. Use `--language` to specify the language\"\n                )\n            mel_segment = pad_or_trim(mel, N_FRAMES).to(model.device).to(dtype)\n            _, probs = model.detect_language(mel_segment)\n            decode_options[\"language\"] = max(probs, key=probs.get)\n            if verbose is not None:\n                print(\n                    f\"Detected language: {LANGUAGES[decode_options['language']].title()}\"\n                )\n    language: str = decode_options[\"language\"]\n    task: str = decode_options.get(\"task\", \"transcribe\")\n    tokenizer = get_tokenizer(\n        model.is_multilingual,\n        num_languages=model.num_languages,\n        language=language,\n        task=task,\n    )\n    if isinstance(clip_timestamps, str):\n        clip_timestamps = [\n            float(ts) for ts in (clip_timestamps.split(\",\") if clip_timestamps else [])\n        ]\n    seek_points: List[int] = [round(ts * FRAMES_PER_SECOND) for ts in clip_timestamps]\n    if len(seek_points) == 0:",
        "type": "code",
        "location": "/whisper/transcribe.py:143-167"
    },
    "333": {
        "file_id": 22,
        "content": "The code is detecting the language of an audio clip using the first 30 seconds and then using that detected language to decide which tokenizer to use for further processing.",
        "type": "comment"
    },
    "334": {
        "file_id": 22,
        "content": "        seek_points.append(0)\n    if len(seek_points) % 2 == 1:\n        seek_points.append(content_frames)\n    seek_clips: List[Tuple[int, int]] = list(zip(seek_points[::2], seek_points[1::2]))\n    punctuation = \"\\\"'“¿([{-\\\"'.。,，!！?？:：”)]}、\"\n    if word_timestamps and task == \"translate\":\n        warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n    def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n        temperatures = (\n            [temperature] if isinstance(temperature, (int, float)) else temperature\n        )\n        decode_result = None\n        for t in temperatures:\n            kwargs = {**decode_options}\n            if t > 0:\n                # disable beam_size and patience when t > 0\n                kwargs.pop(\"beam_size\", None)\n                kwargs.pop(\"patience\", None)\n            else:\n                # disable best_of when t == 0\n                kwargs.pop(\"best_of\", None)\n            options = DecodingOptions(**kwargs, temperature=t)\n            decode_result = model.decode(segment, options)",
        "type": "code",
        "location": "/whisper/transcribe.py:168-195"
    },
    "335": {
        "file_id": 22,
        "content": "This code segment is preparing data for decoding a model. It creates seek points, a list of punctuation, and sets up temperature values for the decoder. If word-level timestamps are present and the task is translation, it warns about potential unreliability. The function `decode_with_fallback` is defined to decode the segment with different temperatures using the specified decoding options and model.",
        "type": "comment"
    },
    "336": {
        "file_id": 22,
        "content": "            needs_fallback = False\n            if (\n                compression_ratio_threshold is not None\n                and decode_result.compression_ratio > compression_ratio_threshold\n            ):\n                needs_fallback = True  # too repetitive\n            if (\n                logprob_threshold is not None\n                and decode_result.avg_logprob < logprob_threshold\n            ):\n                needs_fallback = True  # average log probability is too low\n            if (\n                no_speech_threshold is not None\n                and decode_result.no_speech_prob > no_speech_threshold\n            ):\n                needs_fallback = False  # silence\n            if not needs_fallback:\n                break\n        return decode_result\n    clip_idx = 0\n    seek = seek_clips[clip_idx][0]\n    input_stride = exact_div(\n        N_FRAMES, model.dims.n_audio_ctx\n    )  # mel frames per output token: 2\n    time_precision = (\n        input_stride * HOP_LENGTH / SAMPLE_RATE\n    )  # time per output token: 0.02 (seconds)",
        "type": "code",
        "location": "/whisper/transcribe.py:197-225"
    },
    "337": {
        "file_id": 22,
        "content": "This code checks if the decoding result requires a fallback, based on three conditions: compression ratio, average log probability, and silence detection. If no fallback is needed, it breaks out of the loop. The code then calculates the number of mel frames per output token and time precision for further processing.",
        "type": "comment"
    },
    "338": {
        "file_id": 22,
        "content": "    all_tokens = []\n    all_segments = []\n    prompt_reset_since = 0\n    if initial_prompt is not None:\n        initial_prompt_tokens = tokenizer.encode(\" \" + initial_prompt.strip())\n        all_tokens.extend(initial_prompt_tokens)\n    else:\n        initial_prompt_tokens = []\n    def new_segment(\n        *, start: float, end: float, tokens: torch.Tensor, result: DecodingResult\n    ):\n        tokens = tokens.tolist()\n        text_tokens = [token for token in tokens if token < tokenizer.eot]\n        return {\n            \"seek\": seek,\n            \"start\": start,\n            \"end\": end,\n            \"text\": tokenizer.decode(text_tokens),\n            \"tokens\": tokens,\n            \"temperature\": result.temperature,\n            \"avg_logprob\": result.avg_logprob,\n            \"compression_ratio\": result.compression_ratio,\n            \"no_speech_prob\": result.no_speech_prob,\n        }\n    # show the progress bar when verbose is False (if True, transcribed text will be printed)\n    with tqdm.tqdm(\n        total=content_frames, unit=\"frames\", disable=verbose is not False",
        "type": "code",
        "location": "/whisper/transcribe.py:226-255"
    },
    "339": {
        "file_id": 22,
        "content": "This code segment initializes variables for storing tokens and segments, handles an optional initial prompt, defines a function for creating new speech segments, and sets up a progress bar.",
        "type": "comment"
    },
    "340": {
        "file_id": 22,
        "content": "    ) as pbar:\n        last_speech_timestamp = 0.0\n        # NOTE: This loop is obscurely flattened to make the diff readable.\n        # A later commit should turn this into a simpler nested loop.\n        # for seek_clip_start, seek_clip_end in seek_clips:\n        #     while seek < seek_clip_end\n        while clip_idx < len(seek_clips):\n            seek_clip_start, seek_clip_end = seek_clips[clip_idx]\n            if seek < seek_clip_start:\n                seek = seek_clip_start\n            if seek >= seek_clip_end:\n                clip_idx += 1\n                if clip_idx < len(seek_clips):\n                    seek = seek_clips[clip_idx][0]\n                continue\n            time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n            window_end_time = float((seek + N_FRAMES) * HOP_LENGTH / SAMPLE_RATE)\n            segment_size = min(N_FRAMES, content_frames - seek, seek_clip_end - seek)\n            mel_segment = mel[:, seek : seek + segment_size]\n            segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE",
        "type": "code",
        "location": "/whisper/transcribe.py:256-275"
    },
    "341": {
        "file_id": 22,
        "content": "Iterates over seek clips, finding the next segment to transcribe based on seek time and clip boundaries.",
        "type": "comment"
    },
    "342": {
        "file_id": 22,
        "content": "            mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n            decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n            result: DecodingResult = decode_with_fallback(mel_segment)\n            tokens = torch.tensor(result.tokens)\n            if no_speech_threshold is not None:\n                # no voice activity check\n                should_skip = result.no_speech_prob > no_speech_threshold\n                if (\n                    logprob_threshold is not None\n                    and result.avg_logprob > logprob_threshold\n                ):\n                    # don't skip if the logprob is high enough, despite the no_speech_prob\n                    should_skip = False\n                if should_skip:\n                    seek += segment_size  # fast-forward to the next segment boundary\n                    continue\n            previous_seek = seek\n            current_segments = []\n            # anomalous words are very long/short/improbable\n            def word_anomaly_score(word: dict) -> float:",
        "type": "code",
        "location": "/whisper/transcribe.py:276-300"
    },
    "343": {
        "file_id": 22,
        "content": "This code segment is responsible for speech recognition and processing in the whisper library. It takes an input audio segment, performs speech recognition using a model, and checks if there is any no-speech activity or anomalous words. If any of these conditions are met, it skips the current segment and moves to the next one. The code also keeps track of the seek position (seek) for fast forwarding to the next segment boundary.",
        "type": "comment"
    },
    "344": {
        "file_id": 22,
        "content": "                probability = word.get(\"probability\", 0.0)\n                duration = word[\"end\"] - word[\"start\"]\n                score = 0.0\n                if probability < 0.15:\n                    score += 1.0\n                if duration < 0.133:\n                    score += (0.133 - duration) * 15\n                if duration > 2.0:\n                    score += duration - 2.0\n                return score\n            def is_segment_anomaly(segment: Optional[dict]) -> bool:\n                if segment is None or not segment[\"words\"]:\n                    return False\n                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\n                words = words[:8]\n                score = sum(word_anomaly_score(w) for w in words)\n                return score >= 3 or score + 0.01 >= len(words)\n            def next_words_segment(segments: List[dict]) -> Optional[dict]:\n                return next((s for s in segments if s[\"words\"]), None)\n            timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)",
        "type": "code",
        "location": "/whisper/transcribe.py:301-323"
    },
    "345": {
        "file_id": 22,
        "content": "This code calculates a score for each segment based on word probability and duration, checks if a segment is anomalous by considering its words and returns the next non-empty segment from a list. It also handles timestamp tokens.",
        "type": "comment"
    },
    "346": {
        "file_id": 22,
        "content": "            single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n            consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n            consecutive.add_(1)\n            if len(consecutive) > 0:\n                # if the output contains two consecutive timestamp tokens\n                slices = consecutive.tolist()\n                if single_timestamp_ending:\n                    slices.append(len(tokens))\n                last_slice = 0\n                for current_slice in slices:\n                    sliced_tokens = tokens[last_slice:current_slice]\n                    start_timestamp_pos = (\n                        sliced_tokens[0].item() - tokenizer.timestamp_begin\n                    )\n                    end_timestamp_pos = (\n                        sliced_tokens[-1].item() - tokenizer.timestamp_begin\n                    )\n                    current_segments.append(\n                        new_segment(\n                            start=time_offset + start_timestamp_pos * time_precision,",
        "type": "code",
        "location": "/whisper/transcribe.py:324-345"
    },
    "347": {
        "file_id": 22,
        "content": "This code checks for consecutive timestamp tokens and extracts corresponding segments from the input sequence of tokens. If a segment is found, it calculates the start and end timestamp positions and creates a new segment with adjusted time values.",
        "type": "comment"
    },
    "348": {
        "file_id": 22,
        "content": "                            end=time_offset + end_timestamp_pos * time_precision,\n                            tokens=sliced_tokens,\n                            result=result,\n                        )\n                    )\n                    last_slice = current_slice\n                if single_timestamp_ending:\n                    # single timestamp at the end means no speech after the last timestamp.\n                    seek += segment_size\n                else:\n                    # otherwise, ignore the unfinished segment and seek to the last timestamp\n                    last_timestamp_pos = (\n                        tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n                    )\n                    seek += last_timestamp_pos * input_stride\n            else:\n                duration = segment_duration\n                timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n                if (\n                    len(timestamps) > 0\n                    and timestamps[-1].item() != tokenizer.timestamp_begin",
        "type": "code",
        "location": "/whisper/transcribe.py:346-367"
    },
    "349": {
        "file_id": 22,
        "content": "Code is parsing audio segments and extracting relevant data for further processing. It seeks to the next timestamp or ignores unfinished segments if necessary.",
        "type": "comment"
    },
    "350": {
        "file_id": 22,
        "content": "                ):\n                    # no consecutive timestamps but it has a timestamp; use the last one.\n                    last_timestamp_pos = (\n                        timestamps[-1].item() - tokenizer.timestamp_begin\n                    )\n                    duration = last_timestamp_pos * time_precision\n                current_segments.append(\n                    new_segment(\n                        start=time_offset,\n                        end=time_offset + duration,\n                        tokens=tokens,\n                        result=result,\n                    )\n                )\n                seek += segment_size\n            if word_timestamps:\n                add_word_timestamps(\n                    segments=current_segments,\n                    model=model,\n                    tokenizer=tokenizer,\n                    mel=mel_segment,\n                    num_frames=segment_size,\n                    prepend_punctuations=prepend_punctuations,\n                    append_punctuations=append_punctuations,",
        "type": "code",
        "location": "/whisper/transcribe.py:368-393"
    },
    "351": {
        "file_id": 22,
        "content": "This code segment is responsible for creating speech segments based on the given timestamps and tokens, and then adding word timestamps if necessary. If consecutive timestamps are not found but a timestamp exists, it uses the last one to determine the duration of the current segment. It appends the created segment to the current_segments list, and increments the seek value for the next iteration.",
        "type": "comment"
    },
    "352": {
        "file_id": 22,
        "content": "                    last_speech_timestamp=last_speech_timestamp,\n                )\n                if not single_timestamp_ending:\n                    last_word_end = get_end(current_segments)\n                    if last_word_end is not None and last_word_end > time_offset:\n                        seek = round(last_word_end * FRAMES_PER_SECOND)\n                # skip silence before possible hallucinations\n                if hallucination_silence_threshold is not None:\n                    threshold = hallucination_silence_threshold\n                    if not single_timestamp_ending:\n                        last_word_end = get_end(current_segments)\n                        if last_word_end is not None and last_word_end > time_offset:\n                            remaining_duration = window_end_time - last_word_end\n                            if remaining_duration > threshold:\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\n                            else:\n                                seek = previous_seek + segment_size",
        "type": "code",
        "location": "/whisper/transcribe.py:394-412"
    },
    "353": {
        "file_id": 22,
        "content": "This code skips silence before possible hallucinations and sets seek position accordingly.",
        "type": "comment"
    },
    "354": {
        "file_id": 22,
        "content": "                    # if first segment might be a hallucination, skip leading silence\n                    first_segment = next_words_segment(current_segments)\n                    if first_segment is not None and is_segment_anomaly(first_segment):\n                        gap = first_segment[\"start\"] - time_offset\n                        if gap > threshold:\n                            seek = previous_seek + round(gap * FRAMES_PER_SECOND)\n                            continue\n                    # skip silence before any possible hallucination that is surrounded\n                    # by silence or more hallucinations\n                    hal_last_end = last_speech_timestamp\n                    for si in range(len(current_segments)):\n                        segment = current_segments[si]\n                        if not segment[\"words\"]:\n                            continue\n                        if is_segment_anomaly(segment):\n                            next_segment = next_words_segment(\n                                current_segments[si + 1 :]",
        "type": "code",
        "location": "/whisper/transcribe.py:414-431"
    },
    "355": {
        "file_id": 22,
        "content": "This code skips silence before possible hallucinations that are surrounded by silence or more hallucinations.",
        "type": "comment"
    },
    "356": {
        "file_id": 22,
        "content": "                            )\n                            if next_segment is not None:\n                                hal_next_start = next_segment[\"words\"][0][\"start\"]\n                            else:\n                                hal_next_start = time_offset + segment_duration\n                            silence_before = (\n                                segment[\"start\"] - hal_last_end > threshold\n                                or segment[\"start\"] < threshold\n                                or segment[\"start\"] - time_offset < 2.0\n                            )\n                            silence_after = (\n                                hal_next_start - segment[\"end\"] > threshold\n                                or is_segment_anomaly(next_segment)\n                                or window_end_time - segment[\"end\"] < 2.0\n                            )\n                            if silence_before and silence_after:\n                                seek = round(\n                                    max(time_offset + 1, segment[\"start\"])",
        "type": "code",
        "location": "/whisper/transcribe.py:432-449"
    },
    "357": {
        "file_id": 22,
        "content": "If the next segment exists, set hal_next_start to the start time of the first word in that segment. Otherwise, set it to time_offset plus segment_duration. Determine if there is silence before and after this segment based on certain conditions, such as segment start times or anomalies. If both silences are present, round the seek value to be the maximum between time_offset + 1 and the segment's start time.",
        "type": "comment"
    },
    "358": {
        "file_id": 22,
        "content": "                                    * FRAMES_PER_SECOND\n                                )\n                                if content_duration - segment[\"end\"] < threshold:\n                                    seek = content_frames\n                                current_segments[si:] = []\n                                break\n                        hal_last_end = segment[\"end\"]\n                last_word_end = get_end(current_segments)\n                if last_word_end is not None:\n                    last_speech_timestamp = last_word_end\n            if verbose:\n                for segment in current_segments:\n                    start, end, text = segment[\"start\"], segment[\"end\"], segment[\"text\"]\n                    line = f\"[{format_timestamp(start)} --> {format_timestamp(end)}] {text}\"\n                    print(make_safe(line))\n            # if a segment is instantaneous or does not contain text, clear it\n            for i, segment in enumerate(current_segments):\n                if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":",
        "type": "code",
        "location": "/whisper/transcribe.py:450-470"
    },
    "359": {
        "file_id": 22,
        "content": "Removes small segments and instantaneous segments without text from the current_segments list.",
        "type": "comment"
    },
    "360": {
        "file_id": 22,
        "content": "                    segment[\"text\"] = \"\"\n                    segment[\"tokens\"] = []\n                    segment[\"words\"] = []\n            all_segments.extend(\n                [\n                    {\"id\": i, **segment}\n                    for i, segment in enumerate(\n                        current_segments, start=len(all_segments)\n                    )\n                ]\n            )\n            all_tokens.extend(\n                [token for segment in current_segments for token in segment[\"tokens\"]]\n            )\n            if not condition_on_previous_text or result.temperature > 0.5:\n                # do not feed the prompt tokens if a high temperature was used\n                prompt_reset_since = len(all_tokens)\n            # update progress bar\n            pbar.update(min(content_frames, seek) - previous_seek)\n    return dict(\n        text=tokenizer.decode(all_tokens[len(initial_prompt_tokens) :]),\n        segments=all_segments,\n        language=language,\n    )\ndef cli():\n    from . import available_models",
        "type": "code",
        "location": "/whisper/transcribe.py:471-502"
    },
    "361": {
        "file_id": 22,
        "content": "1. Initializes empty text, tokens, and words for segments.\n2. Appends segments to all_segments list with unique IDs.\n3. Extends all_tokens list by combining segment's tokens.\n4. Checks if condition on previous text or temperature is high; resets prompt_reset_since if true.",
        "type": "comment"
    },
    "362": {
        "file_id": 22,
        "content": "    def valid_model_name(name):\n        if name in available_models() or os.path.exists(name):\n            return name\n        raise ValueError(\n            f\"model should be one of {available_models()} or path to a model checkpoint\"\n        )\n    # fmt: off\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"audio\", nargs=\"+\", type=str, help=\"audio file(s) to transcribe\")\n    parser.add_argument(\"--model\", default=\"small\", type=valid_model_name, help=\"name of the Whisper model to use\")\n    parser.add_argument(\"--model_dir\", type=str, default=None, help=\"the path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n    parser.add_argument(\"--output_format\", \"-f\", type=str, default=\"all\", choi",
        "type": "code",
        "location": "/whisper/transcribe.py:504-518"
    },
    "363": {
        "file_id": 22,
        "content": "The function 'valid_model_name' checks if the input name exists in available models or is a valid path to a model checkpoint. If so, it returns the name. Else, it raises a ValueError with an error message.\n\nThe parser object is initialized for parsing command line arguments. It accepts audio file(s) to transcribe as input and allows specifying various options like model name, model directory, device for PyTorch inference, and output directory for saving the outputs. The default options are set based on system availability.",
        "type": "comment"
    },
    "364": {
        "file_id": 22,
        "content": "ces=[\"txt\", \"vtt\", \"srt\", \"tsv\", \"json\", \"all\"], help=\"format of the output file; if not specified, all available formats will be produced\")\n    parser.add_argument(\"--verbose\", type=str2bool, default=True, help=\"whether to print out the progress and debug messages\")\n    parser.add_argument(\"--task\", type=str, default=\"transcribe\", choices=[\"transcribe\", \"translate\"], help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=None, choices=sorted(LANGUAGES.keys()) + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]), help=\"language spoken in the audio, specify None to perform language detection\")\n    parser.add_argument(\"--temperature\", type=float, default=0, help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=5, help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=5, help=\"number of beams in beam search, only applicable when temperature is zero\")",
        "type": "code",
        "location": "/whisper/transcribe.py:518-526"
    },
    "365": {
        "file_id": 22,
        "content": "This code is parsing command line arguments for a transcribe and translation task. It sets default values and provides options to choose the output file format, enable verbose output, perform speech recognition or translation, specify input language, set temperature for sampling, and set beam size for beam search.",
        "type": "comment"
    },
    "366": {
        "file_id": 22,
        "content": "    parser.add_argument(\"--patience\", type=float, default=None, help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=None, help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default\")\n    parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=True, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")",
        "type": "code",
        "location": "/whisper/transcribe.py:527-532"
    },
    "367": {
        "file_id": 22,
        "content": "This code adds command line arguments to a parser, allowing the user to set optional values for patience, length penalty, tokens to suppress during sampling, initial prompt, and conditioning on previous text.",
        "type": "comment"
    },
    "368": {
        "file_id": 22,
        "content": "    parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n    parser.add_argument(\"--word_timestamps\", type=str2bool,",
        "type": "code",
        "location": "/whisper/transcribe.py:533-539"
    },
    "369": {
        "file_id": 22,
        "content": "This code snippet is adding command-line arguments for various parameters used in a speech transcription process. The parameters include whether to perform inference in half precision (fp16), temperature increment when falling back, compression ratio threshold, log probability threshold, no speech probability threshold, and whether to use word timestamps. These options allow the user to customize how the transcribe function operates.",
        "type": "comment"
    },
    "370": {
        "file_id": 22,
        "content": " default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'“¿([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.。,，!！?？:：”)]}、\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n    parser.add_argument(\"--highlight_words\", type=str2bool, default=False, help=\"(requires --word_timestamps True) underline each word as it is spoken in srt and vtt\")\n    parser.add_argument(\"--max_line_width\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of characters in a line before breaking the line\")\n    parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")\n    parser.add_argument(\"--max_word",
        "type": "code",
        "location": "/whisper/transcribe.py:539-545"
    },
    "371": {
        "file_id": 22,
        "content": "This code is adding optional arguments to a parser for various features related to word-level timestamps and punctuation handling. It includes options for enabling word timestamps, specifying punctuations to merge with next/previous words, highlighting spoken words in output files, and setting maximum line width and count in segments (requires --word_timestamps True).",
        "type": "comment"
    },
    "372": {
        "file_id": 22,
        "content": "s_per_line\", type=optional_int, default=None, help=\"(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment\")\n    parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n    parser.add_argument(\"--clip_timestamps\", type=str, default=\"0\", help=\"comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process, where the last end timestamp defaults to the end of the file\")\n    parser.add_argument(\"--hallucination_silence_threshold\", type=optional_float, help=\"(requires --word_timestamps True) skip silent periods longer than this threshold (in seconds) when a possible hallucination is detected\")\n    # fmt: on\n    args = parser.parse_args().__dict__\n    model_name: str = args.pop(\"model\")\n    model_dir: str = args.pop(\"model_dir\")\n    output_dir: str = args.pop(\"output_dir\")\n    output_format: str = args.pop(\"output_format\")",
        "type": "code",
        "location": "/whisper/transcribe.py:545-555"
    },
    "373": {
        "file_id": 22,
        "content": "The code defines command-line arguments for a transcribe function. It allows setting the maximum number of words in a segment, specifying the number of threads to use for CPU inference, defining clips to process with timestamps, and setting a silence threshold for hallucination detection. The code then parses these arguments into a dictionary.",
        "type": "comment"
    },
    "374": {
        "file_id": 22,
        "content": "    device: str = args.pop(\"device\")\n    os.makedirs(output_dir, exist_ok=True)\n    if model_name.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n        if args[\"language\"] is not None:\n            warnings.warn(\n                f\"{model_name} is an English-only model but receipted '{args['language']}'; using English instead.\"\n            )\n        args[\"language\"] = \"en\"\n    temperature = args.pop(\"temperature\")\n    if (increment := args.pop(\"temperature_increment_on_fallback\")) is not None:\n        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, increment))\n    else:\n        temperature = [temperature]\n    if (threads := args.pop(\"threads\")) > 0:\n        torch.set_num_threads(threads)\n    from . import load_model\n    model = load_model(model_name, device=device, download_root=model_dir)\n    writer = get_writer(output_format, output_dir)\n    word_options = [\n        \"highlight_words\",\n        \"max_line_count\",\n        \"max_line_width\",\n        \"max_words_per_line\",\n    ]\n    if not args[\"word_timestamps\"]:",
        "type": "code",
        "location": "/whisper/transcribe.py:556-586"
    },
    "375": {
        "file_id": 22,
        "content": "The code is setting up the model and its configuration for transcribing audio files. It checks if the received language matches the model's language, warns and defaults to English if not. It also sets the temperature and number of threads for model inference, loads the model, and gets a writer object for output. Additionally, it handles word options such as highlight words, maximum line count, maximum line width, and maximum words per line.",
        "type": "comment"
    },
    "376": {
        "file_id": 22,
        "content": "        for option in word_options:\n            if args[option]:\n                parser.error(f\"--{option} requires --word_timestamps True\")\n    if args[\"max_line_count\"] and not args[\"max_line_width\"]:\n        warnings.warn(\"--max_line_count has no effect without --max_line_width\")\n    if args[\"max_words_per_line\"] and args[\"max_line_width\"]:\n        warnings.warn(\"--max_words_per_line has no effect with --max_line_width\")\n    writer_args = {arg: args.pop(arg) for arg in word_options}\n    for audio_path in args.pop(\"audio\"):\n        try:\n            result = transcribe(model, audio_path, temperature=temperature, **args)\n            writer(result, audio_path, **writer_args)\n        except Exception as e:\n            traceback.print_exc()\n            print(f\"Skipping {audio_path} due to {type(e).__name__}: {str(e)}\")\nif __name__ == \"__main__\":\n    cli()",
        "type": "code",
        "location": "/whisper/transcribe.py:587-605"
    },
    "377": {
        "file_id": 22,
        "content": "This code block checks if certain command line arguments are used correctly and handles any cases where they're not. It then passes the required arguments to the transcribe function and writes the results using a specified writer function.",
        "type": "comment"
    },
    "378": {
        "file_id": 23,
        "content": "/whisper/triton_ops.py",
        "type": "filepath"
    },
    "379": {
        "file_id": 23,
        "content": "The code includes two functions: a dynamic time warping algorithm using Triton's parallel computing and a median filter implemented with CUDA, performing convolution operations on input tensors.",
        "type": "summary"
    },
    "380": {
        "file_id": 23,
        "content": "from functools import lru_cache\nimport numpy as np\nimport torch\ntry:\n    import triton\n    import triton.language as tl\nexcept ImportError:\n    raise RuntimeError(\"triton import failed; try `pip install --pre triton`\")\n@triton.jit\ndef dtw_kernel(\n    cost, trace, x, x_stride, cost_stride, trace_stride, N, M, BLOCK_SIZE: tl.constexpr\n):\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < M\n    for k in range(1, N + M + 1):  # k = i + j\n        tl.debug_barrier()\n        p0 = cost + (k - 1) * cost_stride\n        p1 = cost + k * cost_stride\n        p2 = cost + k * cost_stride + 1\n        c0 = tl.load(p0 + offsets, mask=mask)\n        c1 = tl.load(p1 + offsets, mask=mask)\n        c2 = tl.load(p2 + offsets, mask=mask)\n        x_row = tl.load(x + (k - 1) * x_stride + offsets, mask=mask, other=0)\n        cost_row = x_row + tl.minimum(tl.minimum(c0, c1), c2)\n        cost_ptr = cost + (k + 1) * cost_stride + 1\n        tl.store(cost_ptr + offsets, cost_row, mask=mask)\n        trace_ptr = trace + (k + 1) * trace_stride + 1",
        "type": "code",
        "location": "/whisper/triton_ops.py:1-37"
    },
    "381": {
        "file_id": 23,
        "content": "This code defines a function `dtw_kernel` that uses the Triton programming language to implement dynamic time warping (DTW) algorithm. It takes cost, trace, x arrays as input and calculates the DTW distance between two sequences. The function uses triton's parallel computing capabilities to optimize performance.",
        "type": "comment"
    },
    "382": {
        "file_id": 23,
        "content": "        tl.store(trace_ptr + offsets, 2, mask=mask & (c2 <= c0) & (c2 <= c1))\n        tl.store(trace_ptr + offsets, 1, mask=mask & (c1 <= c0) & (c1 <= c2))\n        tl.store(trace_ptr + offsets, 0, mask=mask & (c0 <= c1) & (c0 <= c2))\n@lru_cache(maxsize=None)\ndef median_kernel(filter_width: int):\n    @triton.jit\n    def kernel(\n        y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr\n    ):  # x.shape[-1] == filter_width\n        row_idx = tl.program_id(0)\n        offsets = tl.arange(0, BLOCK_SIZE)\n        mask = offsets < y_stride\n        x_ptr = x + row_idx * x_stride  # noqa: F841\n        y_ptr = y + row_idx * y_stride\n        LOAD_ALL_ROWS_HERE  # noqa: F821\n        BUBBLESORT_HERE  # noqa: F821\n        tl.store(y_ptr + offsets, MIDDLE_ROW_HERE, mask=mask)  # noqa: F821\n    kernel = triton.JITFunction(kernel.fn)\n    kernel.src = kernel.src.replace(\n        \"    LOAD_ALL_ROWS_HERE\",\n        \"\\n\".join(\n            [\n                f\"    row{i} = tl.load(x_ptr + offsets + {i}, mask=mask)\"\n                for i in range(filter_width)",
        "type": "code",
        "location": "/whisper/triton_ops.py:38-68"
    },
    "383": {
        "file_id": 23,
        "content": "This code is implementing a median filter using Triton library. It loads pixel values from an input image, sorts them, and stores the middle value back into the image. The function is JIT compiled for performance.",
        "type": "comment"
    },
    "384": {
        "file_id": 23,
        "content": "            ]\n        ),\n    )\n    kernel.src = kernel.src.replace(\n        \"    BUBBLESORT_HERE\",\n        \"\\n\\n\".join(\n            [\n                \"\\n\\n\".join(\n                    [\n                        \"\\n\".join(\n                            [\n                                f\"    smaller = tl.where(row{j} < row{j + 1}, row{j}, row{j + 1})\",\n                                f\"    larger = tl.where(row{j} > row{j + 1}, row{j}, row{j + 1})\",\n                                f\"    row{j} = smaller\",\n                                f\"    row{j + 1} = larger\",\n                            ]\n                        )\n                        for j in range(filter_width - i - 1)\n                    ]\n                )\n                for i in range(filter_width // 2 + 1)\n            ]\n        ),\n    )\n    kernel.src = kernel.src.replace(\"MIDDLE_ROW_HERE\", f\"row{filter_width // 2}\")\n    return kernel\ndef median_filter_cuda(x: torch.Tensor, filter_width: int):\n    \"\"\"Apply a median filter of given width along the last dimension of x\"\"\"",
        "type": "code",
        "location": "/whisper/triton_ops.py:69-99"
    },
    "385": {
        "file_id": 23,
        "content": "This code is implementing a median filter with CUDA for a given tensor. It replaces the specified sections of the kernel source code to perform a bubblesort on rows of the tensor, then updates each row with its corresponding median value. The filter_width determines the size of the median filter applied along the last dimension of the input tensor.",
        "type": "comment"
    },
    "386": {
        "file_id": 23,
        "content": "    slices = x.contiguous().unfold(-1, filter_width, 1)\n    grid = np.prod(slices.shape[:-2])\n    kernel = median_kernel(filter_width)\n    y = torch.empty_like(slices[..., 0])\n    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()\n    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)\n    return y",
        "type": "code",
        "location": "/whisper/triton_ops.py:100-109"
    },
    "387": {
        "file_id": 23,
        "content": "This code is performing a convolution operation on an input tensor 'x' using a kernel of size 'filter_width'. The result is stored in tensor 'y', and the BLOCK_SIZE is determined based on the stride of 'x'.",
        "type": "comment"
    },
    "388": {
        "file_id": 24,
        "content": "/whisper/utils.py",
        "type": "filepath"
    },
    "389": {
        "file_id": 24,
        "content": "The code has utility modules, compression ratio calculations, and timestamp formatting for ResultWriter class. It introduces WriteTSV and WriteJSON subclasses for TSV and JSON file formats, along with write_result and get_writer functions.",
        "type": "summary"
    },
    "390": {
        "file_id": 24,
        "content": "import json\nimport os\nimport re\nimport sys\nimport zlib\nfrom typing import Callable, List, Optional, TextIO\nsystem_encoding = sys.getdefaultencoding()\nif system_encoding != \"utf-8\":\n    def make_safe(string):\n        # replaces any character not representable using the system default encoding with an '?',\n        # avoiding UnicodeEncodeError (https://github.com/openai/whisper/discussions/729).\n        return string.encode(system_encoding, errors=\"replace\").decode(system_encoding)\nelse:\n    def make_safe(string):\n        # utf-8 can encode any Unicode code point, so no need to do the round-trip encoding\n        return string\ndef exact_div(x, y):\n    assert x % y == 0\n    return x // y\ndef str2bool(string):\n    str2val = {\"True\": True, \"False\": False}\n    if string in str2val:\n        return str2val[string]\n    else:\n        raise ValueError(f\"Expected one of {set(str2val.keys())}, got {string}\")\ndef optional_int(string):\n    return None if string == \"None\" else int(string)\ndef optional_float(string):\n    return None if string == \"None\" else float(string)",
        "type": "code",
        "location": "/whisper/utils.py:1-42"
    },
    "391": {
        "file_id": 24,
        "content": "The code imports necessary modules, checks the system encoding, and provides several utility functions. It defines make_safe for converting strings to safe format, exact_div for integer division with assertion, str2bool for converting strings to boolean values, optional_int and optional_float for converting strings to respective numeric types if not \"None\".",
        "type": "comment"
    },
    "392": {
        "file_id": 24,
        "content": "def compression_ratio(text) -> float:\n    text_bytes = text.encode(\"utf-8\")\n    return len(text_bytes) / len(zlib.compress(text_bytes))\ndef format_timestamp(\n    seconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n):\n    assert seconds >= 0, \"non-negative timestamp expected\"\n    milliseconds = round(seconds * 1000.0)\n    hours = milliseconds // 3_600_000\n    milliseconds -= hours * 3_600_000\n    minutes = milliseconds // 60_000\n    milliseconds -= minutes * 60_000\n    seconds = milliseconds // 1_000\n    milliseconds -= seconds * 1_000\n    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n    return (\n        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n    )\ndef get_start(segments: List[dict]) -> Optional[float]:\n    return next(\n        (w[\"start\"] for s in segments for w in s[\"words\"]),\n        segments[0][\"start\"] if segments else None,\n    )\ndef get_end(segments: List[dict]) -> Optional[float]:\n    return next(\n        (w[\"end\"] for s in reversed(segments) for w in reversed(s[\"words\"])),",
        "type": "code",
        "location": "/whisper/utils.py:45-80"
    },
    "393": {
        "file_id": 24,
        "content": "1. `compression_ratio` calculates the ratio of the size of compressed text to the original size in bytes.\n2. `format_timestamp` converts a non-negative timestamp into a formatted string, including hours if requested or if they are present.\n3. `get_start` retrieves the start time from the list of segment dictionaries, falling back to the first segment if necessary.\n4. `get_end` retrieves the end time from the last segment in reverse order of segments, or returns None if no segments are present.",
        "type": "comment"
    },
    "394": {
        "file_id": 24,
        "content": "        segments[-1][\"end\"] if segments else None,\n    )\nclass ResultWriter:\n    extension: str\n    def __init__(self, output_dir: str):\n        self.output_dir = output_dir\n    def __call__(\n        self, result: dict, audio_path: str, options: Optional[dict] = None, **kwargs\n    ):\n        audio_basename = os.path.basename(audio_path)\n        audio_basename = os.path.splitext(audio_basename)[0]\n        output_path = os.path.join(\n            self.output_dir, audio_basename + \".\" + self.extension\n        )\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            self.write_result(result, file=f, options=options, **kwargs)\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        raise NotImplementedError\nclass WriteTXT(ResultWriter):\n    extension: str = \"txt\"\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        for segment in result[\"segments\"]:\n            print(segment[\"text\"].strip(), file=file, flush=True)",
        "type": "code",
        "location": "/whisper/utils.py:81-116"
    },
    "395": {
        "file_id": 24,
        "content": "Class \"ResultWriter\" initializes with an output directory and takes a result dictionary, audio path, optional options dictionary, and additional keyword arguments. It writes the result to a file in the specified output directory with the file name based on the audio basename and the class's extension.\n\nThe \"WriteTXT\" class is a subclass of \"ResultWriter\" that writes the result as plain text to a file. For each segment in the result dictionary, it prints the text without leading or trailing whitespace.",
        "type": "comment"
    },
    "396": {
        "file_id": 24,
        "content": "class SubtitlesWriter(ResultWriter):\n    always_include_hours: bool\n    decimal_marker: str\n    def iterate_result(\n        self,\n        result: dict,\n        options: Optional[dict] = None,\n        *,\n        max_line_width: Optional[int] = None,\n        max_line_count: Optional[int] = None,\n        highlight_words: bool = False,\n        max_words_per_line: Optional[int] = None,\n    ):\n        options = options or {}\n        max_line_width = max_line_width or options.get(\"max_line_width\")\n        max_line_count = max_line_count or options.get(\"max_line_count\")\n        highlight_words = highlight_words or options.get(\"highlight_words\", False)\n        max_words_per_line = max_words_per_line or options.get(\"max_words_per_line\")\n        preserve_segments = max_line_count is None or max_line_width is None\n        max_line_width = max_line_width or 1000\n        max_words_per_line = max_words_per_line or 1000\n        def iterate_subtitles():\n            line_len = 0\n            line_count = 1\n            # the next subtitle to yield (a list of word timings with whitespace)",
        "type": "code",
        "location": "/whisper/utils.py:119-145"
    },
    "397": {
        "file_id": 24,
        "content": "This function is setting default values for options and defining an inner function to iterate through subtitles, keeping track of line length and count.",
        "type": "comment"
    },
    "398": {
        "file_id": 24,
        "content": "            subtitle: List[dict] = []\n            last: float = get_start(result[\"segments\"]) or 0.0\n            for segment in result[\"segments\"]:\n                chunk_index = 0\n                words_count = max_words_per_line\n                while chunk_index < len(segment[\"words\"]):\n                    remaining_words = len(segment[\"words\"]) - chunk_index\n                    if max_words_per_line > len(segment[\"words\"]) - chunk_index:\n                        words_count = remaining_words\n                    for i, original_timing in enumerate(\n                        segment[\"words\"][chunk_index : chunk_index + words_count]\n                    ):\n                        timing = original_timing.copy()\n                        long_pause = (\n                            not preserve_segments and timing[\"start\"] - last > 3.0\n                        )\n                        has_room = line_len + len(timing[\"word\"]) <= max_line_width\n                        seg_break = i == 0 and len(subtitle) > 0 and preserve_segments",
        "type": "code",
        "location": "/whisper/utils.py:146-163"
    },
    "399": {
        "file_id": 24,
        "content": "This code is iterating through each segment of text, splitting it into smaller chunks based on maximum words per line, and handling breaks or pauses between segments if necessary.",
        "type": "comment"
    }
}